{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: train_dataset\n",
      "length: 714\n",
      "sample:\n",
      "inputs:\n",
      "dir [0. 0.] (2,)\n",
      "vel [ 0.3418274  -0.25506592] (2,)\n",
      "wall [[1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] (18, 24)\n",
      "outputs:\n",
      "dpos [ 0.303391 -0.285144]\n",
      "vel [ 0.3033905 -0.285141 ]\n",
      "\n",
      "name: test_dataset\n",
      "length: 306\n",
      "sample:\n",
      "inputs:\n",
      "dir [0. 0.] (2,)\n",
      "vel [0. 0.] (2,)\n",
      "wall [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] (18, 24)\n",
      "outputs:\n",
      "dpos [0. 0.]\n",
      "vel [0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from typing import List\n",
    "\n",
    "class MovementDataSet(Dataset):\n",
    "  def __init__(self, data_path: str):\n",
    "    def load_data(type: str):\n",
    "      dir_path: str = os.path.join(data_path, type) # ex: data/inputs\n",
    "      names = []\n",
    "      values = []\n",
    "      for f in os.listdir(dir_path):\n",
    "        names.append(f.replace('.npy', '')) # ex: dash\n",
    "        values.append(np.load(os.path.join(dir_path, f)).astype(np.float32))\n",
    "      return names, values\n",
    "    self.input_names, self.input_values = load_data('input')\n",
    "    self.output_names, self.output_values = load_data('output')\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.output_values[0])\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    def get_line(data):\n",
    "      line = []\n",
    "      for v in data:\n",
    "        line.append(v[idx])\n",
    "      return tuple(line)\n",
    "    return get_line(self.input_values), get_line(self.output_values)\n",
    "    # return tuple(self.input_values[0][idx]), tuple(self.output_values[0][idx])\n",
    "\n",
    "def print_dataset(name, dataset):\n",
    "  print('name:', name)\n",
    "  print('length:', len(dataset))\n",
    "  # print('dtype:', *[e.dtype for e in dataset[0]])\n",
    "  print('sample:')\n",
    "  inputs, outputs = dataset[0]\n",
    "  print('inputs:')\n",
    "  for i, (name, value) in enumerate(zip(move_dataset.input_names, inputs)):\n",
    "    print(name, value, value.shape)\n",
    "  print('outputs:')\n",
    "  for i, (name, value) in enumerate(zip(move_dataset.output_names, outputs)):\n",
    "    print(name, value)\n",
    "  print()\n",
    "\n",
    "move_dataset = MovementDataSet('data/only_side_moves')\n",
    "\n",
    "train_dataset, test_dataset = random_split(move_dataset, [0.7, 0.3], generator=th.Generator().manual_seed(34))\n",
    "\n",
    "print_dataset('train_dataset', train_dataset)\n",
    "print_dataset('test_dataset', test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "th.set_default_dtype(th.float32)\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.wall1 = nn.Linear(18*24, 16)\n",
    "    self.linear1 = nn.Linear(16+2*3, 16)\n",
    "    \n",
    "    self.linear2 = nn.Linear(16+2*2, 2)\n",
    "    \n",
    "    # self.linear3 = nn.Linear(18*24+2*3, 2)\n",
    "    \n",
    "\n",
    "  def forward(self, dir, vel, wall):\n",
    "    # x = self.flatten(wall)\n",
    "    # x = th.cat([dir, jump, vel, x], dim=1)\n",
    "    # x = F.tanh(self.linear3(x))\n",
    "    \n",
    "    x = self.flatten(wall)\n",
    "    x = F.tanh(self.wall1(x))\n",
    "    x = th.cat([dir, vel, x], dim = 1)\n",
    "    x = F.tanh(self.linear1(x))\n",
    "    x = th.cat([dir, vel, x], dim = 1)\n",
    "    x = self.linear2(x)\n",
    "    return (x,)\n",
    "\n",
    "model = MyModel().to(device)\n",
    "\n",
    "# summary(model, input_size=(200,), device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss() # mean absolute error loss\n",
    "optimizer = th.optim.SGD(model.parameters(), lr=0.001, momentum=0.1)\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "  running_loss = 0.\n",
    "  last_loss = 0.\n",
    "\n",
    "  # Here, we use enumerate(training_loader) instead of\n",
    "  # iter(training_loader) so that we can track the batch\n",
    "  # index and do some intra-epoch reporting\n",
    "  for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "    # Every data instance is an input + label pair\n",
    "    inputs = [a.to(device) for a in inputs]\n",
    "    labels = [a.to(device) for a in labels]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(*inputs)\n",
    "    \n",
    "    loss = loss_fn(outputs[0], labels[0])\n",
    "    loss.backward()\n",
    "\n",
    "    # Adjust learning weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # print('batch loss:', loss.item())\n",
    "    # Gather data and report\n",
    "    running_loss += loss.item()\n",
    "    pc = 2 # print count\n",
    "    if i % pc == pc-1:\n",
    "      last_loss = running_loss / pc # loss per batch\n",
    "      # print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "      # tb_x = epoch_index * len(train_dataloader) + i + 1\n",
    "      # tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "      running_loss = 0.\n",
    "\n",
    "  return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vcanaa\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\functional.py:1956: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (256x20 and 22x16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[0;32m     17\u001b[0m model\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 18\u001b[0m avg_loss \u001b[39m=\u001b[39m train_one_epoch(epoch_number, writer)\n\u001b[0;32m     20\u001b[0m \u001b[39m# We don't need gradients on to do reporting\u001b[39;00m\n\u001b[0;32m     21\u001b[0m model\u001b[39m.\u001b[39mtrain(\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[0;32m     18\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     20\u001b[0m \u001b[39m# print(len(inputs))\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m# print(len(inputs[0]))\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m# Make predictions for this batch\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49minputs)\n\u001b[0;32m     25\u001b[0m \u001b[39m# Compute the loss and its gradients\u001b[39;00m\n\u001b[0;32m     26\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs[\u001b[39m0\u001b[39m], labels[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\vcanaa\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m, in \u001b[0;36mMyModel.forward\u001b[1;34m(self, dir, vel, wall)\u001b[0m\n\u001b[0;32m     25\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mtanh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwall1(x))\n\u001b[0;32m     26\u001b[0m x \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mcat([\u001b[39mdir\u001b[39m, vel, x], dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mtanh(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear1(x))\n\u001b[0;32m     28\u001b[0m x \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mcat([\u001b[39mdir\u001b[39m, vel, x], dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     29\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear2(x)\n",
      "File \u001b[1;32mc:\\Users\\vcanaa\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\vcanaa\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (256x20 and 22x16)"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 1000\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "  # Make sure gradient tracking is on, and do a pass over the data\n",
    "  model.train(True)\n",
    "  avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "  # We don't need gradients on to do reporting\n",
    "  model.train(False)\n",
    "\n",
    "  running_vloss = 0.0\n",
    "  for i, (vinputs, vlabels) in enumerate(test_dataloader):\n",
    "    vinputs = [a.to(device) for a in vinputs]\n",
    "    vlabels = [a.to(device) for a in vlabels]\n",
    "    voutputs = model(*vinputs)\n",
    "    vloss = loss_fn(voutputs[0], vlabels[0])\n",
    "    running_vloss += vloss\n",
    "\n",
    "  avg_vloss = running_vloss / (i + 1)\n",
    "  print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "  # Log the running loss averaged per batch\n",
    "  # for both training and validation\n",
    "  writer.add_scalars('Training vs. Validation Loss',\n",
    "                  { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                  epoch_number + 1)\n",
    "  writer.flush()\n",
    "\n",
    "  # Track best performance, and save the model's state\n",
    "  if avg_vloss < best_vloss:\n",
    "    best_vloss = avg_vloss\n",
    "    model_path = 'models/model_{}_{}'.format(timestamp, epoch_number)\n",
    "    th.save(model.state_dict(), model_path)\n",
    "\n",
    "  epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0. 0.] [ 0.3418274  -0.25506592] [[1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "flatten() takes from 0 to 1 positional arguments but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[181], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m*\u001b[39mtrain_dataset[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m----> 3\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m*\u001b[39mmodel(\u001b[39m*\u001b[39;49m[a\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m*\u001b[39;49ma\u001b[39m.\u001b[39;49mshape) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m train_dataset[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m]]))\n\u001b[0;32m      4\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m*\u001b[39mtrain_dataset[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\vcanaa\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[155], line 24\u001b[0m, in \u001b[0;36mMyModel.forward\u001b[1;34m(self, dir, jump, vel, wall)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mdir\u001b[39m, jump, vel, wall):\n\u001b[0;32m     20\u001b[0m   \u001b[39m# x = self.flatten(wall)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m   \u001b[39m# x = th.cat([dir, jump, vel, x], dim=1)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m   \u001b[39m# x = F.tanh(self.linear3(x))\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflatten(wall)\n\u001b[0;32m     25\u001b[0m   x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mtanh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwall1(x))\n\u001b[0;32m     26\u001b[0m   x \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mcat([\u001b[39mdir\u001b[39m, jump, vel, x], dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vcanaa\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\vcanaa\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\flatten.py:46\u001b[0m, in \u001b[0;36mFlatten.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mflatten(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstart_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mend_dim)\n",
      "\u001b[1;31mTypeError\u001b[0m: flatten() takes from 0 to 1 positional arguments but 2 were given"
     ]
    }
   ],
   "source": [
    "print(*train_dataset[0][0])\n",
    "with th.no_grad():\n",
    "  print(*model(*[a.reshape(1, *a.shape) for a in train_dataset[0][0]]))\n",
    "  print(*train_dataset[0][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
