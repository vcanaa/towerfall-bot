{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: train_dataset\n",
      "length: 714\n",
      "sample:\n",
      "inputs:\n",
      "dir [0. 0.] (2,)\n",
      "vel [ 0.3418274  -0.25506592] (2,)\n",
      "wall [[1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] (18, 24)\n",
      "outputs:\n",
      "dpos [ 0.303391 -0.285144]\n",
      "vel [ 0.3033905 -0.285141 ]\n",
      "\n",
      "name: test_dataset\n",
      "length: 306\n",
      "sample:\n",
      "inputs:\n",
      "dir [0. 0.] (2,)\n",
      "vel [0. 0.] (2,)\n",
      "wall [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] (18, 24)\n",
      "outputs:\n",
      "dpos [0. 0.]\n",
      "vel [0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from typing import List\n",
    "\n",
    "class MovementDataSet(Dataset):\n",
    "  def __init__(self, data_path: str):\n",
    "    def load_data(type: str):\n",
    "      dir_path: str = os.path.join(data_path, type) # ex: data/inputs\n",
    "      names = []\n",
    "      values = []\n",
    "      for f in os.listdir(dir_path):\n",
    "        names.append(f.replace('.npy', '')) # ex: dash\n",
    "        values.append(np.load(os.path.join(dir_path, f)).astype(np.float32))\n",
    "      return names, values\n",
    "    self.input_names, self.input_values = load_data('input')\n",
    "    self.output_names, self.output_values = load_data('output')\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.output_values[0])\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    def get_line(data):\n",
    "      line = []\n",
    "      for v in data:\n",
    "        line.append(v[idx])\n",
    "      return tuple(line)\n",
    "    return get_line(self.input_values), get_line(self.output_values)\n",
    "    # return tuple(self.input_values[0][idx]), tuple(self.output_values[0][idx])\n",
    "\n",
    "def print_dataset(name, dataset):\n",
    "  print('name:', name)\n",
    "  print('length:', len(dataset))\n",
    "  # print('dtype:', *[e.dtype for e in dataset[0]])\n",
    "  print('sample:')\n",
    "  inputs, outputs = dataset[0]\n",
    "  print('inputs:')\n",
    "  for i, (name, value) in enumerate(zip(move_dataset.input_names, inputs)):\n",
    "    print(name, value, value.shape)\n",
    "  print('outputs:')\n",
    "  for i, (name, value) in enumerate(zip(move_dataset.output_names, outputs)):\n",
    "    print(name, value)\n",
    "  print()\n",
    "\n",
    "move_dataset = MovementDataSet('data/only_side_moves')\n",
    "\n",
    "train_dataset, test_dataset = random_split(move_dataset, [0.7, 0.3], generator=th.Generator().manual_seed(34))\n",
    "\n",
    "print_dataset('train_dataset', train_dataset)\n",
    "print_dataset('test_dataset', test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "th.set_default_dtype(th.float32)\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.wall1 = nn.Linear(18*24, 16)\n",
    "    self.linear1 = nn.Linear(16+2*2, 16)\n",
    "    \n",
    "    self.linear2 = nn.Linear(16+2*2, 2)\n",
    "    \n",
    "    # self.linear3 = nn.Linear(18*24+2*3, 2)\n",
    "    \n",
    "\n",
    "  def forward(self, dir, vel, wall):\n",
    "    # x = self.flatten(wall)\n",
    "    # x = th.cat([dir, jump, vel, x], dim=1)\n",
    "    # x = F.tanh(self.linear3(x))\n",
    "    \n",
    "    x = self.flatten(wall)\n",
    "    x = F.tanh(self.wall1(x))\n",
    "    x = th.cat([dir, vel, x], dim = 1)\n",
    "    x = F.tanh(self.linear1(x))\n",
    "    x = th.cat([dir, vel, x], dim = 1)\n",
    "    x = self.linear2(x)\n",
    "    return (x,)\n",
    "\n",
    "model = MyModel().to(device)\n",
    "\n",
    "# summary(model, input_size=(200,), device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss() # mean absolute error loss\n",
    "optimizer = th.optim.SGD(model.parameters(), lr=0.001, momentum=0.1)\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "  running_loss = 0.\n",
    "  last_loss = 0.\n",
    "\n",
    "  # Here, we use enumerate(training_loader) instead of\n",
    "  # iter(training_loader) so that we can track the batch\n",
    "  # index and do some intra-epoch reporting\n",
    "  for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "    # Every data instance is an input + label pair\n",
    "    inputs = [a.to(device) for a in inputs]\n",
    "    labels = [a.to(device) for a in labels]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(*inputs)\n",
    "    \n",
    "    loss = loss_fn(outputs[0], labels[0])\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "    pc = 2 # print count\n",
    "    if i % pc == pc-1:\n",
    "      last_loss = running_loss / pc # loss per batch\n",
    "      # print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "      # tb_x = epoch_index * len(train_dataloader) + i + 1\n",
    "      # tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "      running_loss = 0.\n",
    "\n",
    "  return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "LOSS train 0.25601696968078613 valid 0.22008764743804932\n",
      "EPOCH 2:\n",
      "LOSS train 0.25955501943826675 valid 0.22420138120651245\n",
      "EPOCH 3:\n",
      "LOSS train 0.24759326875209808 valid 0.20477694272994995\n",
      "EPOCH 4:\n",
      "LOSS train 0.2484312206506729 valid 0.24935950338840485\n",
      "EPOCH 5:\n",
      "LOSS train 0.24499906599521637 valid 0.22009804844856262\n",
      "EPOCH 6:\n",
      "LOSS train 0.23643898963928223 valid 0.23708736896514893\n",
      "EPOCH 7:\n",
      "LOSS train 0.23659075051546097 valid 0.21618914604187012\n",
      "EPOCH 8:\n",
      "LOSS train 0.23106984794139862 valid 0.21491798758506775\n",
      "EPOCH 9:\n",
      "LOSS train 0.22937316447496414 valid 0.21140511333942413\n",
      "EPOCH 10:\n",
      "LOSS train 0.24363167583942413 valid 0.20293793082237244\n",
      "EPOCH 11:\n",
      "LOSS train 0.23567894846200943 valid 0.20434942841529846\n",
      "EPOCH 12:\n",
      "LOSS train 0.2222091406583786 valid 0.2123916894197464\n",
      "EPOCH 13:\n",
      "LOSS train 0.21674732118844986 valid 0.21531015634536743\n",
      "EPOCH 14:\n",
      "LOSS train 0.223968043923378 valid 0.2320706844329834\n",
      "EPOCH 15:\n",
      "LOSS train 0.23070906847715378 valid 0.2086724042892456\n",
      "EPOCH 16:\n",
      "LOSS train 0.2314179763197899 valid 0.21226894855499268\n",
      "EPOCH 17:\n",
      "LOSS train 0.2175394669175148 valid 0.21683499217033386\n",
      "EPOCH 18:\n",
      "LOSS train 0.21996864676475525 valid 0.22072310745716095\n",
      "EPOCH 19:\n",
      "LOSS train 0.23861876130104065 valid 0.20756804943084717\n",
      "EPOCH 20:\n",
      "LOSS train 0.2305772453546524 valid 0.20251265168190002\n",
      "EPOCH 21:\n",
      "LOSS train 0.2175769805908203 valid 0.22408956289291382\n",
      "EPOCH 22:\n",
      "LOSS train 0.2305050790309906 valid 0.20866671204566956\n",
      "EPOCH 23:\n",
      "LOSS train 0.23857159912586212 valid 0.18873795866966248\n",
      "EPOCH 24:\n",
      "LOSS train 0.2187921702861786 valid 0.24474826455116272\n",
      "EPOCH 25:\n",
      "LOSS train 0.22458522021770477 valid 0.20572875440120697\n",
      "EPOCH 26:\n",
      "LOSS train 0.2217114046216011 valid 0.18756255507469177\n",
      "EPOCH 27:\n",
      "LOSS train 0.22690103948116302 valid 0.22608932852745056\n",
      "EPOCH 28:\n",
      "LOSS train 0.22444786131381989 valid 0.19075781106948853\n",
      "EPOCH 29:\n",
      "LOSS train 0.23120813816785812 valid 0.2015475630760193\n",
      "EPOCH 30:\n",
      "LOSS train 0.22326623648405075 valid 0.18920858204364777\n",
      "EPOCH 31:\n",
      "LOSS train 0.21699565649032593 valid 0.20434609055519104\n",
      "EPOCH 32:\n",
      "LOSS train 0.22737812995910645 valid 0.19202817976474762\n",
      "EPOCH 33:\n",
      "LOSS train 0.21540279686450958 valid 0.20801645517349243\n",
      "EPOCH 34:\n",
      "LOSS train 0.21215758472681046 valid 0.23192767798900604\n",
      "EPOCH 35:\n",
      "LOSS train 0.22203315049409866 valid 0.2321626991033554\n",
      "EPOCH 36:\n",
      "LOSS train 0.22474385052919388 valid 0.19137300550937653\n",
      "EPOCH 37:\n",
      "LOSS train 0.21300268173217773 valid 0.20845982432365417\n",
      "EPOCH 38:\n",
      "LOSS train 0.21823911368846893 valid 0.20002317428588867\n",
      "EPOCH 39:\n",
      "LOSS train 0.2194158360362053 valid 0.22238722443580627\n",
      "EPOCH 40:\n",
      "LOSS train 0.20140577852725983 valid 0.20161239802837372\n",
      "EPOCH 41:\n",
      "LOSS train 0.22337192296981812 valid 0.17947623133659363\n",
      "EPOCH 42:\n",
      "LOSS train 0.21363642066717148 valid 0.21828001737594604\n",
      "EPOCH 43:\n",
      "LOSS train 0.21772268414497375 valid 0.17474281787872314\n",
      "EPOCH 44:\n",
      "LOSS train 0.2221374437212944 valid 0.2221233993768692\n",
      "EPOCH 45:\n",
      "LOSS train 0.22085364162921906 valid 0.22498555481433868\n",
      "EPOCH 46:\n",
      "LOSS train 0.21064826846122742 valid 0.2181423008441925\n",
      "EPOCH 47:\n",
      "LOSS train 0.21442744880914688 valid 0.22546862065792084\n",
      "EPOCH 48:\n",
      "LOSS train 0.21941713988780975 valid 0.20791411399841309\n",
      "EPOCH 49:\n",
      "LOSS train 0.21071739494800568 valid 0.2091793417930603\n",
      "EPOCH 50:\n",
      "LOSS train 0.21502327173948288 valid 0.1890849620103836\n",
      "EPOCH 51:\n",
      "LOSS train 0.20826447010040283 valid 0.21006245911121368\n",
      "EPOCH 52:\n",
      "LOSS train 0.22141699492931366 valid 0.21432951092720032\n",
      "EPOCH 53:\n",
      "LOSS train 0.210968017578125 valid 0.22104127705097198\n",
      "EPOCH 54:\n",
      "LOSS train 0.20398536324501038 valid 0.21299219131469727\n",
      "EPOCH 55:\n",
      "LOSS train 0.22010990977287292 valid 0.17919301986694336\n",
      "EPOCH 56:\n",
      "LOSS train 0.21761837601661682 valid 0.19682803750038147\n",
      "EPOCH 57:\n",
      "LOSS train 0.21364116668701172 valid 0.19466564059257507\n",
      "EPOCH 58:\n",
      "LOSS train 0.2179984152317047 valid 0.2108832448720932\n",
      "EPOCH 59:\n",
      "LOSS train 0.21388046443462372 valid 0.191353902220726\n",
      "EPOCH 60:\n",
      "LOSS train 0.20172718912363052 valid 0.20084148645401\n",
      "EPOCH 61:\n",
      "LOSS train 0.2104354202747345 valid 0.20538002252578735\n",
      "EPOCH 62:\n",
      "LOSS train 0.21664414554834366 valid 0.18635448813438416\n",
      "EPOCH 63:\n",
      "LOSS train 0.21835561096668243 valid 0.2074633538722992\n",
      "EPOCH 64:\n",
      "LOSS train 0.20912568271160126 valid 0.20203639566898346\n",
      "EPOCH 65:\n",
      "LOSS train 0.2120734006166458 valid 0.17381170392036438\n",
      "EPOCH 66:\n",
      "LOSS train 0.21531279385089874 valid 0.1818525493144989\n",
      "EPOCH 67:\n",
      "LOSS train 0.21950586885213852 valid 0.19781747460365295\n",
      "EPOCH 68:\n",
      "LOSS train 0.19946922361850739 valid 0.1967647671699524\n",
      "EPOCH 69:\n",
      "LOSS train 0.20046024024486542 valid 0.2463742196559906\n",
      "EPOCH 70:\n",
      "LOSS train 0.20989402383565903 valid 0.1973705142736435\n",
      "EPOCH 71:\n",
      "LOSS train 0.2093527615070343 valid 0.22151538729667664\n",
      "EPOCH 72:\n",
      "LOSS train 0.20372417569160461 valid 0.21080423891544342\n",
      "EPOCH 73:\n",
      "LOSS train 0.21370726823806763 valid 0.19494976103305817\n",
      "EPOCH 74:\n",
      "LOSS train 0.19156098365783691 valid 0.21391718089580536\n",
      "EPOCH 75:\n",
      "LOSS train 0.19756624847650528 valid 0.17357394099235535\n",
      "EPOCH 76:\n",
      "LOSS train 0.2086520567536354 valid 0.17240619659423828\n",
      "EPOCH 77:\n",
      "LOSS train 0.20276861637830734 valid 0.20334573090076447\n",
      "EPOCH 78:\n",
      "LOSS train 0.2082209587097168 valid 0.20269492268562317\n",
      "EPOCH 79:\n",
      "LOSS train 0.20621945708990097 valid 0.17225928604602814\n",
      "EPOCH 80:\n",
      "LOSS train 0.19667302072048187 valid 0.17857402563095093\n",
      "EPOCH 81:\n",
      "LOSS train 0.20471013337373734 valid 0.21645717322826385\n",
      "EPOCH 82:\n",
      "LOSS train 0.20993633568286896 valid 0.1954282820224762\n",
      "EPOCH 83:\n",
      "LOSS train 0.2023039162158966 valid 0.18165424466133118\n",
      "EPOCH 84:\n",
      "LOSS train 0.19601432234048843 valid 0.22768406569957733\n",
      "EPOCH 85:\n",
      "LOSS train 0.20234976708889008 valid 0.17892639338970184\n",
      "EPOCH 86:\n",
      "LOSS train 0.20563487708568573 valid 0.17700837552547455\n",
      "EPOCH 87:\n",
      "LOSS train 0.198176771402359 valid 0.18738923966884613\n",
      "EPOCH 88:\n",
      "LOSS train 0.20508786290884018 valid 0.16746613383293152\n",
      "EPOCH 89:\n",
      "LOSS train 0.19198372960090637 valid 0.19601941108703613\n",
      "EPOCH 90:\n",
      "LOSS train 0.2064792737364769 valid 0.17142872512340546\n",
      "EPOCH 91:\n",
      "LOSS train 0.20059099793434143 valid 0.2002888023853302\n",
      "EPOCH 92:\n",
      "LOSS train 0.20051822066307068 valid 0.1995718628168106\n",
      "EPOCH 93:\n",
      "LOSS train 0.19960740208625793 valid 0.18247249722480774\n",
      "EPOCH 94:\n",
      "LOSS train 0.18981481343507767 valid 0.20158250629901886\n",
      "EPOCH 95:\n",
      "LOSS train 0.20277651399374008 valid 0.1772969663143158\n",
      "EPOCH 96:\n",
      "LOSS train 0.21116787940263748 valid 0.16240379214286804\n",
      "EPOCH 97:\n",
      "LOSS train 0.1927027553319931 valid 0.17475035786628723\n",
      "EPOCH 98:\n",
      "LOSS train 0.20423340797424316 valid 0.1818222999572754\n",
      "EPOCH 99:\n",
      "LOSS train 0.20362195372581482 valid 0.21301063895225525\n",
      "EPOCH 100:\n",
      "LOSS train 0.1993439793586731 valid 0.16306859254837036\n",
      "EPOCH 101:\n",
      "LOSS train 0.196244478225708 valid 0.17203736305236816\n",
      "EPOCH 102:\n",
      "LOSS train 0.19947449862957 valid 0.1807982325553894\n",
      "EPOCH 103:\n",
      "LOSS train 0.20144639909267426 valid 0.20957759022712708\n",
      "EPOCH 104:\n",
      "LOSS train 0.19374235719442368 valid 0.17135167121887207\n",
      "EPOCH 105:\n",
      "LOSS train 0.19739363342523575 valid 0.15107718110084534\n",
      "EPOCH 106:\n",
      "LOSS train 0.19725662469863892 valid 0.17119470238685608\n",
      "EPOCH 107:\n",
      "LOSS train 0.20416375994682312 valid 0.20049309730529785\n",
      "EPOCH 108:\n",
      "LOSS train 0.1958804428577423 valid 0.1767805814743042\n",
      "EPOCH 109:\n",
      "LOSS train 0.19383827596902847 valid 0.20661425590515137\n",
      "EPOCH 110:\n",
      "LOSS train 0.1948057934641838 valid 0.17172643542289734\n",
      "EPOCH 111:\n",
      "LOSS train 0.19422289729118347 valid 0.19089701771736145\n",
      "EPOCH 112:\n",
      "LOSS train 0.19759486615657806 valid 0.17034819722175598\n",
      "EPOCH 113:\n",
      "LOSS train 0.20698759704828262 valid 0.19759753346443176\n",
      "EPOCH 114:\n",
      "LOSS train 0.1981872394680977 valid 0.17471083998680115\n",
      "EPOCH 115:\n",
      "LOSS train 0.19640439003705978 valid 0.15965643525123596\n",
      "EPOCH 116:\n",
      "LOSS train 0.20303825289011002 valid 0.169070303440094\n",
      "EPOCH 117:\n",
      "LOSS train 0.18753699958324432 valid 0.2071225345134735\n",
      "EPOCH 118:\n",
      "LOSS train 0.19015146046876907 valid 0.17699259519577026\n",
      "EPOCH 119:\n",
      "LOSS train 0.19470907747745514 valid 0.17358389496803284\n",
      "EPOCH 120:\n",
      "LOSS train 0.18541064113378525 valid 0.1726866364479065\n",
      "EPOCH 121:\n",
      "LOSS train 0.19510432332754135 valid 0.17285926640033722\n",
      "EPOCH 122:\n",
      "LOSS train 0.1989983320236206 valid 0.19009774923324585\n",
      "EPOCH 123:\n",
      "LOSS train 0.19218790531158447 valid 0.18205738067626953\n",
      "EPOCH 124:\n",
      "LOSS train 0.1922408565878868 valid 0.19579622149467468\n",
      "EPOCH 125:\n",
      "LOSS train 0.1787218153476715 valid 0.18702003359794617\n",
      "EPOCH 126:\n",
      "LOSS train 0.19143706560134888 valid 0.18063275516033173\n",
      "EPOCH 127:\n",
      "LOSS train 0.19375712424516678 valid 0.1591329276561737\n",
      "EPOCH 128:\n",
      "LOSS train 0.19840390235185623 valid 0.16125977039337158\n",
      "EPOCH 129:\n",
      "LOSS train 0.1812739446759224 valid 0.1600467413663864\n",
      "EPOCH 130:\n",
      "LOSS train 0.1904553920030594 valid 0.19402380287647247\n",
      "EPOCH 131:\n",
      "LOSS train 0.19618381559848785 valid 0.18976350128650665\n",
      "EPOCH 132:\n",
      "LOSS train 0.18055622279644012 valid 0.18222546577453613\n",
      "EPOCH 133:\n",
      "LOSS train 0.18822594732046127 valid 0.16370582580566406\n",
      "EPOCH 134:\n",
      "LOSS train 0.1923426166176796 valid 0.1944957971572876\n",
      "EPOCH 135:\n",
      "LOSS train 0.1866779550909996 valid 0.1603071093559265\n",
      "EPOCH 136:\n",
      "LOSS train 0.19432388991117477 valid 0.17793840169906616\n",
      "EPOCH 137:\n",
      "LOSS train 0.1865280121564865 valid 0.15180142223834991\n",
      "EPOCH 138:\n",
      "LOSS train 0.1800784021615982 valid 0.17718040943145752\n",
      "EPOCH 139:\n",
      "LOSS train 0.1879030093550682 valid 0.1717934012413025\n",
      "EPOCH 140:\n",
      "LOSS train 0.18747245520353317 valid 0.14979234337806702\n",
      "EPOCH 141:\n",
      "LOSS train 0.19395462423563004 valid 0.18992279469966888\n",
      "EPOCH 142:\n",
      "LOSS train 0.18267518281936646 valid 0.1657121181488037\n",
      "EPOCH 143:\n",
      "LOSS train 0.16836737096309662 valid 0.18217499554157257\n",
      "EPOCH 144:\n",
      "LOSS train 0.18374914675951004 valid 0.195110023021698\n",
      "EPOCH 145:\n",
      "LOSS train 0.19259385019540787 valid 0.16825903952121735\n",
      "EPOCH 146:\n",
      "LOSS train 0.1752518266439438 valid 0.16015388071537018\n",
      "EPOCH 147:\n",
      "LOSS train 0.1890980526804924 valid 0.17788729071617126\n",
      "EPOCH 148:\n",
      "LOSS train 0.18901830166578293 valid 0.16682633757591248\n",
      "EPOCH 149:\n",
      "LOSS train 0.1784689500927925 valid 0.17667683959007263\n",
      "EPOCH 150:\n",
      "LOSS train 0.18645407259464264 valid 0.14453819394111633\n",
      "EPOCH 151:\n",
      "LOSS train 0.17700792104005814 valid 0.16122201085090637\n",
      "EPOCH 152:\n",
      "LOSS train 0.18772591650485992 valid 0.18773409724235535\n",
      "EPOCH 153:\n",
      "LOSS train 0.19478291273117065 valid 0.21129977703094482\n",
      "EPOCH 154:\n",
      "LOSS train 0.1776382029056549 valid 0.17609074711799622\n",
      "EPOCH 155:\n",
      "LOSS train 0.1881410777568817 valid 0.16470077633857727\n",
      "EPOCH 156:\n",
      "LOSS train 0.1900976598262787 valid 0.15864339470863342\n",
      "EPOCH 157:\n",
      "LOSS train 0.17941564321517944 valid 0.19360202550888062\n",
      "EPOCH 158:\n",
      "LOSS train 0.17720294743776321 valid 0.17506319284439087\n",
      "EPOCH 159:\n",
      "LOSS train 0.17552679777145386 valid 0.15317867696285248\n",
      "EPOCH 160:\n",
      "LOSS train 0.1807028353214264 valid 0.1756996214389801\n",
      "EPOCH 161:\n",
      "LOSS train 0.18367771804332733 valid 0.2009814977645874\n",
      "EPOCH 162:\n",
      "LOSS train 0.1851826012134552 valid 0.1692613959312439\n",
      "EPOCH 163:\n",
      "LOSS train 0.17541351914405823 valid 0.19086697697639465\n",
      "EPOCH 164:\n",
      "LOSS train 0.18122044950723648 valid 0.17129944264888763\n",
      "EPOCH 165:\n",
      "LOSS train 0.17636851966381073 valid 0.17621228098869324\n",
      "EPOCH 166:\n",
      "LOSS train 0.17946060001850128 valid 0.20520669221878052\n",
      "EPOCH 167:\n",
      "LOSS train 0.18607471138238907 valid 0.17005005478858948\n",
      "EPOCH 168:\n",
      "LOSS train 0.1796140968799591 valid 0.17784538865089417\n",
      "EPOCH 169:\n",
      "LOSS train 0.18258382380008698 valid 0.18709930777549744\n",
      "EPOCH 170:\n",
      "LOSS train 0.17938372492790222 valid 0.15825031697750092\n",
      "EPOCH 171:\n",
      "LOSS train 0.18208104372024536 valid 0.15348951518535614\n",
      "EPOCH 172:\n",
      "LOSS train 0.17639601230621338 valid 0.1846625804901123\n",
      "EPOCH 173:\n",
      "LOSS train 0.18488045036792755 valid 0.17161518335342407\n",
      "EPOCH 174:\n",
      "LOSS train 0.18600060045719147 valid 0.18573534488677979\n",
      "EPOCH 175:\n",
      "LOSS train 0.17528437077999115 valid 0.15032562613487244\n",
      "EPOCH 176:\n",
      "LOSS train 0.1734263300895691 valid 0.17347881197929382\n",
      "EPOCH 177:\n",
      "LOSS train 0.17808592319488525 valid 0.18183022737503052\n",
      "EPOCH 178:\n",
      "LOSS train 0.1732332557439804 valid 0.16683870553970337\n",
      "EPOCH 179:\n",
      "LOSS train 0.18168459087610245 valid 0.17069673538208008\n",
      "EPOCH 180:\n",
      "LOSS train 0.1766251027584076 valid 0.18799936771392822\n",
      "EPOCH 181:\n",
      "LOSS train 0.17840345948934555 valid 0.1629490852355957\n",
      "EPOCH 182:\n",
      "LOSS train 0.16507279127836227 valid 0.17910254001617432\n",
      "EPOCH 183:\n",
      "LOSS train 0.17572730034589767 valid 0.17429199814796448\n",
      "EPOCH 184:\n",
      "LOSS train 0.185368150472641 valid 0.17396581172943115\n",
      "EPOCH 185:\n",
      "LOSS train 0.17364740371704102 valid 0.163330078125\n",
      "EPOCH 186:\n",
      "LOSS train 0.16734453290700912 valid 0.16539785265922546\n",
      "EPOCH 187:\n",
      "LOSS train 0.17297405749559402 valid 0.1691037118434906\n",
      "EPOCH 188:\n",
      "LOSS train 0.1679036021232605 valid 0.15106064081192017\n",
      "EPOCH 189:\n",
      "LOSS train 0.17031142115592957 valid 0.1758858859539032\n",
      "EPOCH 190:\n",
      "LOSS train 0.1789880245923996 valid 0.1732855588197708\n",
      "EPOCH 191:\n",
      "LOSS train 0.1672295480966568 valid 0.1920691728591919\n",
      "EPOCH 192:\n",
      "LOSS train 0.17034215480089188 valid 0.14440566301345825\n",
      "EPOCH 193:\n",
      "LOSS train 0.17600345611572266 valid 0.17319247126579285\n",
      "EPOCH 194:\n",
      "LOSS train 0.1641736626625061 valid 0.16779817640781403\n",
      "EPOCH 195:\n",
      "LOSS train 0.1687149927020073 valid 0.1611945778131485\n",
      "EPOCH 196:\n",
      "LOSS train 0.17166072875261307 valid 0.19122043251991272\n",
      "EPOCH 197:\n",
      "LOSS train 0.17009513825178146 valid 0.16877228021621704\n",
      "EPOCH 198:\n",
      "LOSS train 0.16959694772958755 valid 0.16505417227745056\n",
      "EPOCH 199:\n",
      "LOSS train 0.17803557962179184 valid 0.1468498855829239\n",
      "EPOCH 200:\n",
      "LOSS train 0.17828360199928284 valid 0.16063342988491058\n",
      "EPOCH 201:\n",
      "LOSS train 0.18144825100898743 valid 0.17152711749076843\n",
      "EPOCH 202:\n",
      "LOSS train 0.17109742760658264 valid 0.13591250777244568\n",
      "EPOCH 203:\n",
      "LOSS train 0.16700781881809235 valid 0.16438411176204681\n",
      "EPOCH 204:\n",
      "LOSS train 0.16834723949432373 valid 0.18428269028663635\n",
      "EPOCH 205:\n",
      "LOSS train 0.17497603595256805 valid 0.1684749871492386\n",
      "EPOCH 206:\n",
      "LOSS train 0.16547486931085587 valid 0.14905613660812378\n",
      "EPOCH 207:\n",
      "LOSS train 0.1828516274690628 valid 0.15820623934268951\n",
      "EPOCH 208:\n",
      "LOSS train 0.16874471306800842 valid 0.15066802501678467\n",
      "EPOCH 209:\n",
      "LOSS train 0.16966135799884796 valid 0.1389811635017395\n",
      "EPOCH 210:\n",
      "LOSS train 0.16756803542375565 valid 0.16997653245925903\n",
      "EPOCH 211:\n",
      "LOSS train 0.18138252943754196 valid 0.15832661092281342\n",
      "EPOCH 212:\n",
      "LOSS train 0.1651948019862175 valid 0.14980994164943695\n",
      "EPOCH 213:\n",
      "LOSS train 0.1681249588727951 valid 0.16256430745124817\n",
      "EPOCH 214:\n",
      "LOSS train 0.155761256814003 valid 0.15933799743652344\n",
      "EPOCH 215:\n",
      "LOSS train 0.1724521517753601 valid 0.1675874888896942\n",
      "EPOCH 216:\n",
      "LOSS train 0.15847033262252808 valid 0.17307224869728088\n",
      "EPOCH 217:\n",
      "LOSS train 0.17608195543289185 valid 0.17295610904693604\n",
      "EPOCH 218:\n",
      "LOSS train 0.1692139208316803 valid 0.16582578420639038\n",
      "EPOCH 219:\n",
      "LOSS train 0.16221275180578232 valid 0.15018784999847412\n",
      "EPOCH 220:\n",
      "LOSS train 0.17044838517904282 valid 0.16031122207641602\n",
      "EPOCH 221:\n",
      "LOSS train 0.18030531704425812 valid 0.14928144216537476\n",
      "EPOCH 222:\n",
      "LOSS train 0.16020932793617249 valid 0.1866709291934967\n",
      "EPOCH 223:\n",
      "LOSS train 0.17150422930717468 valid 0.16024860739707947\n",
      "EPOCH 224:\n",
      "LOSS train 0.1701422929763794 valid 0.1423836350440979\n",
      "EPOCH 225:\n",
      "LOSS train 0.17437437176704407 valid 0.18174414336681366\n",
      "EPOCH 226:\n",
      "LOSS train 0.1685921475291252 valid 0.1570502519607544\n",
      "EPOCH 227:\n",
      "LOSS train 0.17119356244802475 valid 0.16805684566497803\n",
      "EPOCH 228:\n",
      "LOSS train 0.162888303399086 valid 0.1606220006942749\n",
      "EPOCH 229:\n",
      "LOSS train 0.16420455276966095 valid 0.13548539578914642\n",
      "EPOCH 230:\n",
      "LOSS train 0.16770993173122406 valid 0.18989446759223938\n",
      "EPOCH 231:\n",
      "LOSS train 0.1745264083147049 valid 0.15839099884033203\n",
      "EPOCH 232:\n",
      "LOSS train 0.17025187611579895 valid 0.15772011876106262\n",
      "EPOCH 233:\n",
      "LOSS train 0.17180082947015762 valid 0.14585372805595398\n",
      "EPOCH 234:\n",
      "LOSS train 0.1847548708319664 valid 0.13211779296398163\n",
      "EPOCH 235:\n",
      "LOSS train 0.15598172694444656 valid 0.15482991933822632\n",
      "EPOCH 236:\n",
      "LOSS train 0.1624394655227661 valid 0.13475677371025085\n",
      "EPOCH 237:\n",
      "LOSS train 0.16122660040855408 valid 0.17116352915763855\n",
      "EPOCH 238:\n",
      "LOSS train 0.16197820752859116 valid 0.17230507731437683\n",
      "EPOCH 239:\n",
      "LOSS train 0.16788310557603836 valid 0.1608007401227951\n",
      "EPOCH 240:\n",
      "LOSS train 0.16561377048492432 valid 0.1535886824131012\n",
      "EPOCH 241:\n",
      "LOSS train 0.16324123740196228 valid 0.1580401062965393\n",
      "EPOCH 242:\n",
      "LOSS train 0.15594780445098877 valid 0.16741997003555298\n",
      "EPOCH 243:\n",
      "LOSS train 0.17108987271785736 valid 0.1778620481491089\n",
      "EPOCH 244:\n",
      "LOSS train 0.16417522728443146 valid 0.1813700944185257\n",
      "EPOCH 245:\n",
      "LOSS train 0.17355625331401825 valid 0.17717096209526062\n",
      "EPOCH 246:\n",
      "LOSS train 0.1577133610844612 valid 0.1681429147720337\n",
      "EPOCH 247:\n",
      "LOSS train 0.16208314895629883 valid 0.14507995545864105\n",
      "EPOCH 248:\n",
      "LOSS train 0.17582795768976212 valid 0.1521569788455963\n",
      "EPOCH 249:\n",
      "LOSS train 0.15108650922775269 valid 0.17572236061096191\n",
      "EPOCH 250:\n",
      "LOSS train 0.17231321334838867 valid 0.15377110242843628\n",
      "EPOCH 251:\n",
      "LOSS train 0.15309588611125946 valid 0.16969284415245056\n",
      "EPOCH 252:\n",
      "LOSS train 0.16161026805639267 valid 0.18057066202163696\n",
      "EPOCH 253:\n",
      "LOSS train 0.15936322510242462 valid 0.1474209874868393\n",
      "EPOCH 254:\n",
      "LOSS train 0.16785140335559845 valid 0.15419089794158936\n",
      "EPOCH 255:\n",
      "LOSS train 0.16743329167366028 valid 0.17078682780265808\n",
      "EPOCH 256:\n",
      "LOSS train 0.1604500114917755 valid 0.1679588407278061\n",
      "EPOCH 257:\n",
      "LOSS train 0.16578994691371918 valid 0.1412847340106964\n",
      "EPOCH 258:\n",
      "LOSS train 0.1703399121761322 valid 0.1567503809928894\n",
      "EPOCH 259:\n",
      "LOSS train 0.15524086356163025 valid 0.14846768975257874\n",
      "EPOCH 260:\n",
      "LOSS train 0.1541404202580452 valid 0.1526111364364624\n",
      "EPOCH 261:\n",
      "LOSS train 0.15879248082637787 valid 0.16847844421863556\n",
      "EPOCH 262:\n",
      "LOSS train 0.16190731525421143 valid 0.1523129642009735\n",
      "EPOCH 263:\n",
      "LOSS train 0.16204173862934113 valid 0.157582625746727\n",
      "EPOCH 264:\n",
      "LOSS train 0.16411231458187103 valid 0.17188450694084167\n",
      "EPOCH 265:\n",
      "LOSS train 0.1662406325340271 valid 0.16462695598602295\n",
      "EPOCH 266:\n",
      "LOSS train 0.16706602275371552 valid 0.14825859665870667\n",
      "EPOCH 267:\n",
      "LOSS train 0.1608831211924553 valid 0.15865084528923035\n",
      "EPOCH 268:\n",
      "LOSS train 0.1581239253282547 valid 0.1526932418346405\n",
      "EPOCH 269:\n",
      "LOSS train 0.1613823026418686 valid 0.1493866890668869\n",
      "EPOCH 270:\n",
      "LOSS train 0.15783415734767914 valid 0.14571034908294678\n",
      "EPOCH 271:\n",
      "LOSS train 0.1586422175168991 valid 0.1668928861618042\n",
      "EPOCH 272:\n",
      "LOSS train 0.1618678793311119 valid 0.15304401516914368\n",
      "EPOCH 273:\n",
      "LOSS train 0.15694093704223633 valid 0.18833599984645844\n",
      "EPOCH 274:\n",
      "LOSS train 0.1580074355006218 valid 0.15204501152038574\n",
      "EPOCH 275:\n",
      "LOSS train 0.1634155660867691 valid 0.15550872683525085\n",
      "EPOCH 276:\n",
      "LOSS train 0.16465501487255096 valid 0.1740657389163971\n",
      "EPOCH 277:\n",
      "LOSS train 0.1637507751584053 valid 0.15126723051071167\n",
      "EPOCH 278:\n",
      "LOSS train 0.1583729162812233 valid 0.15081992745399475\n",
      "EPOCH 279:\n",
      "LOSS train 0.15498530864715576 valid 0.1636662632226944\n",
      "EPOCH 280:\n",
      "LOSS train 0.15657706558704376 valid 0.13644510507583618\n",
      "EPOCH 281:\n",
      "LOSS train 0.1585695892572403 valid 0.18591514229774475\n",
      "EPOCH 282:\n",
      "LOSS train 0.15969430655241013 valid 0.1360819935798645\n",
      "EPOCH 283:\n",
      "LOSS train 0.1559293419122696 valid 0.14890486001968384\n",
      "EPOCH 284:\n",
      "LOSS train 0.15239881724119186 valid 0.12875254452228546\n",
      "EPOCH 285:\n",
      "LOSS train 0.16730137169361115 valid 0.1629171371459961\n",
      "EPOCH 286:\n",
      "LOSS train 0.1543407142162323 valid 0.1333230435848236\n",
      "EPOCH 287:\n",
      "LOSS train 0.1514907032251358 valid 0.18562082946300507\n",
      "EPOCH 288:\n",
      "LOSS train 0.15901587903499603 valid 0.17397770285606384\n",
      "EPOCH 289:\n",
      "LOSS train 0.15661615133285522 valid 0.1573297083377838\n",
      "EPOCH 290:\n",
      "LOSS train 0.1541558876633644 valid 0.14158610999584198\n",
      "EPOCH 291:\n",
      "LOSS train 0.1578761637210846 valid 0.14683867990970612\n",
      "EPOCH 292:\n",
      "LOSS train 0.15256021916866302 valid 0.15382978320121765\n",
      "EPOCH 293:\n",
      "LOSS train 0.1672041043639183 valid 0.13331151008605957\n",
      "EPOCH 294:\n",
      "LOSS train 0.14975925534963608 valid 0.15779078006744385\n",
      "EPOCH 295:\n",
      "LOSS train 0.15514801442623138 valid 0.14155972003936768\n",
      "EPOCH 296:\n",
      "LOSS train 0.16235997527837753 valid 0.14442354440689087\n",
      "EPOCH 297:\n",
      "LOSS train 0.15767085552215576 valid 0.16532815992832184\n",
      "EPOCH 298:\n",
      "LOSS train 0.15073316544294357 valid 0.16460677981376648\n",
      "EPOCH 299:\n",
      "LOSS train 0.1556950956583023 valid 0.1338786780834198\n",
      "EPOCH 300:\n",
      "LOSS train 0.15898141264915466 valid 0.11409705877304077\n",
      "EPOCH 301:\n",
      "LOSS train 0.15301884710788727 valid 0.14542926847934723\n",
      "EPOCH 302:\n",
      "LOSS train 0.1525806039571762 valid 0.1302654892206192\n",
      "EPOCH 303:\n",
      "LOSS train 0.15335798263549805 valid 0.16928888857364655\n",
      "EPOCH 304:\n",
      "LOSS train 0.14928863942623138 valid 0.1683872789144516\n",
      "EPOCH 305:\n",
      "LOSS train 0.15272100269794464 valid 0.13824743032455444\n",
      "EPOCH 306:\n",
      "LOSS train 0.14736635982990265 valid 0.15735268592834473\n",
      "EPOCH 307:\n",
      "LOSS train 0.15309133753180504 valid 0.13430455327033997\n",
      "EPOCH 308:\n",
      "LOSS train 0.15206310153007507 valid 0.14810022711753845\n",
      "EPOCH 309:\n",
      "LOSS train 0.151480033993721 valid 0.15983796119689941\n",
      "EPOCH 310:\n",
      "LOSS train 0.15599428117275238 valid 0.15682289004325867\n",
      "EPOCH 311:\n",
      "LOSS train 0.1514686569571495 valid 0.14114153385162354\n",
      "EPOCH 312:\n",
      "LOSS train 0.15466267615556717 valid 0.15481595695018768\n",
      "EPOCH 313:\n",
      "LOSS train 0.15874238312244415 valid 0.1308288723230362\n",
      "EPOCH 314:\n",
      "LOSS train 0.1516968011856079 valid 0.16160421073436737\n",
      "EPOCH 315:\n",
      "LOSS train 0.1569700986146927 valid 0.1506391167640686\n",
      "EPOCH 316:\n",
      "LOSS train 0.1570064201951027 valid 0.13431599736213684\n",
      "EPOCH 317:\n",
      "LOSS train 0.15283852815628052 valid 0.13361403346061707\n",
      "EPOCH 318:\n",
      "LOSS train 0.1511167585849762 valid 0.1564413458108902\n",
      "EPOCH 319:\n",
      "LOSS train 0.16287899017333984 valid 0.15170887112617493\n",
      "EPOCH 320:\n",
      "LOSS train 0.15154284238815308 valid 0.13495798408985138\n",
      "EPOCH 321:\n",
      "LOSS train 0.15034477412700653 valid 0.13980433344841003\n",
      "EPOCH 322:\n",
      "LOSS train 0.1480293720960617 valid 0.14400728046894073\n",
      "EPOCH 323:\n",
      "LOSS train 0.1570444405078888 valid 0.13986694812774658\n",
      "EPOCH 324:\n",
      "LOSS train 0.15195952355861664 valid 0.1554698646068573\n",
      "EPOCH 325:\n",
      "LOSS train 0.14902381598949432 valid 0.13344845175743103\n",
      "EPOCH 326:\n",
      "LOSS train 0.14920824766159058 valid 0.156900554895401\n",
      "EPOCH 327:\n",
      "LOSS train 0.15148745477199554 valid 0.12490533292293549\n",
      "EPOCH 328:\n",
      "LOSS train 0.15443936735391617 valid 0.16362135112285614\n",
      "EPOCH 329:\n",
      "LOSS train 0.14514467120170593 valid 0.15127810835838318\n",
      "EPOCH 330:\n",
      "LOSS train 0.14749892055988312 valid 0.13734376430511475\n",
      "EPOCH 331:\n",
      "LOSS train 0.15120317041873932 valid 0.15509791672229767\n",
      "EPOCH 332:\n",
      "LOSS train 0.14710821211338043 valid 0.1441294550895691\n",
      "EPOCH 333:\n",
      "LOSS train 0.15881779789924622 valid 0.13242439925670624\n",
      "EPOCH 334:\n",
      "LOSS train 0.147821806371212 valid 0.14178398251533508\n",
      "EPOCH 335:\n",
      "LOSS train 0.1415707692503929 valid 0.14179693162441254\n",
      "EPOCH 336:\n",
      "LOSS train 0.14916442334651947 valid 0.13490945100784302\n",
      "EPOCH 337:\n",
      "LOSS train 0.14908884465694427 valid 0.15257307887077332\n",
      "EPOCH 338:\n",
      "LOSS train 0.14913856983184814 valid 0.14460107684135437\n",
      "EPOCH 339:\n",
      "LOSS train 0.1512175276875496 valid 0.1424991488456726\n",
      "EPOCH 340:\n",
      "LOSS train 0.1395934522151947 valid 0.13307681679725647\n",
      "EPOCH 341:\n",
      "LOSS train 0.13611076772212982 valid 0.12742207944393158\n",
      "EPOCH 342:\n",
      "LOSS train 0.14000099897384644 valid 0.13418734073638916\n",
      "EPOCH 343:\n",
      "LOSS train 0.14547663182020187 valid 0.1244133785367012\n",
      "EPOCH 344:\n",
      "LOSS train 0.15244747698307037 valid 0.13776622712612152\n",
      "EPOCH 345:\n",
      "LOSS train 0.1496785432100296 valid 0.13566148281097412\n",
      "EPOCH 346:\n",
      "LOSS train 0.1464504525065422 valid 0.1285344362258911\n",
      "EPOCH 347:\n",
      "LOSS train 0.14240092039108276 valid 0.13582421839237213\n",
      "EPOCH 348:\n",
      "LOSS train 0.14473898708820343 valid 0.1512783318758011\n",
      "EPOCH 349:\n",
      "LOSS train 0.14924434572458267 valid 0.1561223566532135\n",
      "EPOCH 350:\n",
      "LOSS train 0.1533929631114006 valid 0.13973385095596313\n",
      "EPOCH 351:\n",
      "LOSS train 0.1500093638896942 valid 0.13235589861869812\n",
      "EPOCH 352:\n",
      "LOSS train 0.14776165783405304 valid 0.13755276799201965\n",
      "EPOCH 353:\n",
      "LOSS train 0.14401903003454208 valid 0.15433187782764435\n",
      "EPOCH 354:\n",
      "LOSS train 0.14744752645492554 valid 0.14280667901039124\n",
      "EPOCH 355:\n",
      "LOSS train 0.14111606031656265 valid 0.1322917342185974\n",
      "EPOCH 356:\n",
      "LOSS train 0.14175856858491898 valid 0.13261905312538147\n",
      "EPOCH 357:\n",
      "LOSS train 0.14333322644233704 valid 0.13628649711608887\n",
      "EPOCH 358:\n",
      "LOSS train 0.15180085599422455 valid 0.14401382207870483\n",
      "EPOCH 359:\n",
      "LOSS train 0.14455103129148483 valid 0.1303052306175232\n",
      "EPOCH 360:\n",
      "LOSS train 0.14379850775003433 valid 0.15063591301441193\n",
      "EPOCH 361:\n",
      "LOSS train 0.14860652387142181 valid 0.1493227630853653\n",
      "EPOCH 362:\n",
      "LOSS train 0.13871218264102936 valid 0.16343331336975098\n",
      "EPOCH 363:\n",
      "LOSS train 0.14927826821804047 valid 0.13406246900558472\n",
      "EPOCH 364:\n",
      "LOSS train 0.15032712370157242 valid 0.12889142334461212\n",
      "EPOCH 365:\n",
      "LOSS train 0.1471702605485916 valid 0.12159872055053711\n",
      "EPOCH 366:\n",
      "LOSS train 0.13785555213689804 valid 0.1327194720506668\n",
      "EPOCH 367:\n",
      "LOSS train 0.14809484779834747 valid 0.1346341073513031\n",
      "EPOCH 368:\n",
      "LOSS train 0.15202989429235458 valid 0.12544876337051392\n",
      "EPOCH 369:\n",
      "LOSS train 0.14812131226062775 valid 0.1483447551727295\n",
      "EPOCH 370:\n",
      "LOSS train 0.14491187036037445 valid 0.13157230615615845\n",
      "EPOCH 371:\n",
      "LOSS train 0.13534484058618546 valid 0.16075679659843445\n",
      "EPOCH 372:\n",
      "LOSS train 0.13440190255641937 valid 0.14933866262435913\n",
      "EPOCH 373:\n",
      "LOSS train 0.14211548119783401 valid 0.13076098263263702\n",
      "EPOCH 374:\n",
      "LOSS train 0.14960984885692596 valid 0.12755155563354492\n",
      "EPOCH 375:\n",
      "LOSS train 0.15509963035583496 valid 0.16942667961120605\n",
      "EPOCH 376:\n",
      "LOSS train 0.1346307247877121 valid 0.11556094884872437\n",
      "EPOCH 377:\n",
      "LOSS train 0.1405014768242836 valid 0.12780100107192993\n",
      "EPOCH 378:\n",
      "LOSS train 0.14520472288131714 valid 0.11661180108785629\n",
      "EPOCH 379:\n",
      "LOSS train 0.14229688048362732 valid 0.13118217885494232\n",
      "EPOCH 380:\n",
      "LOSS train 0.14158960431814194 valid 0.14112260937690735\n",
      "EPOCH 381:\n",
      "LOSS train 0.1409817636013031 valid 0.14572617411613464\n",
      "EPOCH 382:\n",
      "LOSS train 0.1424884796142578 valid 0.15024349093437195\n",
      "EPOCH 383:\n",
      "LOSS train 0.1426592320203781 valid 0.13933181762695312\n",
      "EPOCH 384:\n",
      "LOSS train 0.1492340862751007 valid 0.14397960901260376\n",
      "EPOCH 385:\n",
      "LOSS train 0.14466676861047745 valid 0.12581275403499603\n",
      "EPOCH 386:\n",
      "LOSS train 0.1386094093322754 valid 0.14921043813228607\n",
      "EPOCH 387:\n",
      "LOSS train 0.13173913210630417 valid 0.13360299170017242\n",
      "EPOCH 388:\n",
      "LOSS train 0.15336747467517853 valid 0.12485665082931519\n",
      "EPOCH 389:\n",
      "LOSS train 0.1432095319032669 valid 0.14641812443733215\n",
      "EPOCH 390:\n",
      "LOSS train 0.13671322911977768 valid 0.16174134612083435\n",
      "EPOCH 391:\n",
      "LOSS train 0.13981729745864868 valid 0.13676360249519348\n",
      "EPOCH 392:\n",
      "LOSS train 0.14054197072982788 valid 0.13430863618850708\n",
      "EPOCH 393:\n",
      "LOSS train 0.13883115351200104 valid 0.13571800291538239\n",
      "EPOCH 394:\n",
      "LOSS train 0.1318100392818451 valid 0.12146556377410889\n",
      "EPOCH 395:\n",
      "LOSS train 0.13491953909397125 valid 0.1393395960330963\n",
      "EPOCH 396:\n",
      "LOSS train 0.13104228302836418 valid 0.1300753504037857\n",
      "EPOCH 397:\n",
      "LOSS train 0.1363109052181244 valid 0.1229982078075409\n",
      "EPOCH 398:\n",
      "LOSS train 0.1324499547481537 valid 0.1583380550146103\n",
      "EPOCH 399:\n",
      "LOSS train 0.1416681483387947 valid 0.13477720320224762\n",
      "EPOCH 400:\n",
      "LOSS train 0.1363166868686676 valid 0.1647827923297882\n",
      "EPOCH 401:\n",
      "LOSS train 0.13075358420610428 valid 0.12545138597488403\n",
      "EPOCH 402:\n",
      "LOSS train 0.12894586473703384 valid 0.14852750301361084\n",
      "EPOCH 403:\n",
      "LOSS train 0.1280278041958809 valid 0.11481409519910812\n",
      "EPOCH 404:\n",
      "LOSS train 0.13513661175966263 valid 0.11194539070129395\n",
      "EPOCH 405:\n",
      "LOSS train 0.1380138397216797 valid 0.129145547747612\n",
      "EPOCH 406:\n",
      "LOSS train 0.1341756135225296 valid 0.1296350657939911\n",
      "EPOCH 407:\n",
      "LOSS train 0.14102592319250107 valid 0.12761259078979492\n",
      "EPOCH 408:\n",
      "LOSS train 0.13956370949745178 valid 0.12582257390022278\n",
      "EPOCH 409:\n",
      "LOSS train 0.13264529407024384 valid 0.12153133749961853\n",
      "EPOCH 410:\n",
      "LOSS train 0.14531436562538147 valid 0.13105547428131104\n",
      "EPOCH 411:\n",
      "LOSS train 0.13696552067995071 valid 0.13810265064239502\n",
      "EPOCH 412:\n",
      "LOSS train 0.13962619751691818 valid 0.11400377005338669\n",
      "EPOCH 413:\n",
      "LOSS train 0.13640961796045303 valid 0.15550586581230164\n",
      "EPOCH 414:\n",
      "LOSS train 0.14680835604667664 valid 0.1381809115409851\n",
      "EPOCH 415:\n",
      "LOSS train 0.139561727643013 valid 0.13029801845550537\n",
      "EPOCH 416:\n",
      "LOSS train 0.14033126831054688 valid 0.13117095828056335\n",
      "EPOCH 417:\n",
      "LOSS train 0.13577277213335037 valid 0.12356346100568771\n",
      "EPOCH 418:\n",
      "LOSS train 0.13154495507478714 valid 0.10740277171134949\n",
      "EPOCH 419:\n",
      "LOSS train 0.14080005884170532 valid 0.14185012876987457\n",
      "EPOCH 420:\n",
      "LOSS train 0.13580777496099472 valid 0.13813412189483643\n",
      "EPOCH 421:\n",
      "LOSS train 0.13398447632789612 valid 0.1266980618238449\n",
      "EPOCH 422:\n",
      "LOSS train 0.13163821399211884 valid 0.1236322671175003\n",
      "EPOCH 423:\n",
      "LOSS train 0.1417190209031105 valid 0.14032447338104248\n",
      "EPOCH 424:\n",
      "LOSS train 0.12979640066623688 valid 0.13590437173843384\n",
      "EPOCH 425:\n",
      "LOSS train 0.1324361115694046 valid 0.13165467977523804\n",
      "EPOCH 426:\n",
      "LOSS train 0.13730968534946442 valid 0.12666018307209015\n",
      "EPOCH 427:\n",
      "LOSS train 0.1373932808637619 valid 0.1375562846660614\n",
      "EPOCH 428:\n",
      "LOSS train 0.12726883217692375 valid 0.1292484998703003\n",
      "EPOCH 429:\n",
      "LOSS train 0.13876108825206757 valid 0.1242213100194931\n",
      "EPOCH 430:\n",
      "LOSS train 0.12982351332902908 valid 0.14461824297904968\n",
      "EPOCH 431:\n",
      "LOSS train 0.13341668248176575 valid 0.12386611849069595\n",
      "EPOCH 432:\n",
      "LOSS train 0.1407656967639923 valid 0.13686323165893555\n",
      "EPOCH 433:\n",
      "LOSS train 0.12601668015122414 valid 0.11929182708263397\n",
      "EPOCH 434:\n",
      "LOSS train 0.1361696794629097 valid 0.12768922746181488\n",
      "EPOCH 435:\n",
      "LOSS train 0.1327609196305275 valid 0.13998547196388245\n",
      "EPOCH 436:\n",
      "LOSS train 0.12921978905797005 valid 0.14906978607177734\n",
      "EPOCH 437:\n",
      "LOSS train 0.1304178163409233 valid 0.12065534293651581\n",
      "EPOCH 438:\n",
      "LOSS train 0.14052021503448486 valid 0.10711517930030823\n",
      "EPOCH 439:\n",
      "LOSS train 0.14059679955244064 valid 0.14974769949913025\n",
      "EPOCH 440:\n",
      "LOSS train 0.1310647316277027 valid 0.10263679921627045\n",
      "EPOCH 441:\n",
      "LOSS train 0.1304623857140541 valid 0.11407645046710968\n",
      "EPOCH 442:\n",
      "LOSS train 0.13197938352823257 valid 0.1391656994819641\n",
      "EPOCH 443:\n",
      "LOSS train 0.11856812238693237 valid 0.11028845608234406\n",
      "EPOCH 444:\n",
      "LOSS train 0.13517257571220398 valid 0.12040968984365463\n",
      "EPOCH 445:\n",
      "LOSS train 0.12227877974510193 valid 0.13359181582927704\n",
      "EPOCH 446:\n",
      "LOSS train 0.13137338310480118 valid 0.1508145034313202\n",
      "EPOCH 447:\n",
      "LOSS train 0.12604205682873726 valid 0.12580081820487976\n",
      "EPOCH 448:\n",
      "LOSS train 0.12559664994478226 valid 0.10826174914836884\n",
      "EPOCH 449:\n",
      "LOSS train 0.1334976926445961 valid 0.11650293320417404\n",
      "EPOCH 450:\n",
      "LOSS train 0.13041914999485016 valid 0.12225723266601562\n",
      "EPOCH 451:\n",
      "LOSS train 0.12194749712944031 valid 0.15479311347007751\n",
      "EPOCH 452:\n",
      "LOSS train 0.1316968947649002 valid 0.14284557104110718\n",
      "EPOCH 453:\n",
      "LOSS train 0.13288619369268417 valid 0.13876019418239594\n",
      "EPOCH 454:\n",
      "LOSS train 0.12921005487442017 valid 0.14198100566864014\n",
      "EPOCH 455:\n",
      "LOSS train 0.12128609418869019 valid 0.1042913943529129\n",
      "EPOCH 456:\n",
      "LOSS train 0.12737488746643066 valid 0.13629263639450073\n",
      "EPOCH 457:\n",
      "LOSS train 0.12532082572579384 valid 0.1359013020992279\n",
      "EPOCH 458:\n",
      "LOSS train 0.13221991807222366 valid 0.1408316195011139\n",
      "EPOCH 459:\n",
      "LOSS train 0.12417244911193848 valid 0.1265595257282257\n",
      "EPOCH 460:\n",
      "LOSS train 0.11958780884742737 valid 0.11725372076034546\n",
      "EPOCH 461:\n",
      "LOSS train 0.12587441131472588 valid 0.10353787243366241\n",
      "EPOCH 462:\n",
      "LOSS train 0.1328267976641655 valid 0.11727547645568848\n",
      "EPOCH 463:\n",
      "LOSS train 0.12955158203840256 valid 0.12813420593738556\n",
      "EPOCH 464:\n",
      "LOSS train 0.12160550057888031 valid 0.11597190797328949\n",
      "EPOCH 465:\n",
      "LOSS train 0.12931443750858307 valid 0.1211126446723938\n",
      "EPOCH 466:\n",
      "LOSS train 0.12780001014471054 valid 0.12715351581573486\n",
      "EPOCH 467:\n",
      "LOSS train 0.11676319316029549 valid 0.13118866086006165\n",
      "EPOCH 468:\n",
      "LOSS train 0.12674108892679214 valid 0.1452884078025818\n",
      "EPOCH 469:\n",
      "LOSS train 0.12703493982553482 valid 0.10921669006347656\n",
      "EPOCH 470:\n",
      "LOSS train 0.11970362067222595 valid 0.14463463425636292\n",
      "EPOCH 471:\n",
      "LOSS train 0.12803690880537033 valid 0.11804436892271042\n",
      "EPOCH 472:\n",
      "LOSS train 0.11110039055347443 valid 0.12406690418720245\n",
      "EPOCH 473:\n",
      "LOSS train 0.12191769480705261 valid 0.11262549459934235\n",
      "EPOCH 474:\n",
      "LOSS train 0.12241868674755096 valid 0.10288016498088837\n",
      "EPOCH 475:\n",
      "LOSS train 0.12054195255041122 valid 0.13328662514686584\n",
      "EPOCH 476:\n",
      "LOSS train 0.13035548478364944 valid 0.13154348731040955\n",
      "EPOCH 477:\n",
      "LOSS train 0.1312684752047062 valid 0.1502586305141449\n",
      "EPOCH 478:\n",
      "LOSS train 0.12777601182460785 valid 0.1284835785627365\n",
      "EPOCH 479:\n",
      "LOSS train 0.12336426973342896 valid 0.1152239590883255\n",
      "EPOCH 480:\n",
      "LOSS train 0.12400837615132332 valid 0.11054974794387817\n",
      "EPOCH 481:\n",
      "LOSS train 0.11049041524529457 valid 0.11566698551177979\n",
      "EPOCH 482:\n",
      "LOSS train 0.1239747703075409 valid 0.12576700747013092\n",
      "EPOCH 483:\n",
      "LOSS train 0.12307729572057724 valid 0.11910596489906311\n",
      "EPOCH 484:\n",
      "LOSS train 0.1321195475757122 valid 0.11176048219203949\n",
      "EPOCH 485:\n",
      "LOSS train 0.13437367230653763 valid 0.1412721872329712\n",
      "EPOCH 486:\n",
      "LOSS train 0.12922389805316925 valid 0.11872114986181259\n",
      "EPOCH 487:\n",
      "LOSS train 0.12711980938911438 valid 0.12611344456672668\n",
      "EPOCH 488:\n",
      "LOSS train 0.13308531790971756 valid 0.11870656162500381\n",
      "EPOCH 489:\n",
      "LOSS train 0.11913026124238968 valid 0.14170971512794495\n",
      "EPOCH 490:\n",
      "LOSS train 0.12568357959389687 valid 0.1384245902299881\n",
      "EPOCH 491:\n",
      "LOSS train 0.1245473138988018 valid 0.10881991684436798\n",
      "EPOCH 492:\n",
      "LOSS train 0.1322067305445671 valid 0.11383441090583801\n",
      "EPOCH 493:\n",
      "LOSS train 0.12280083075165749 valid 0.1174556240439415\n",
      "EPOCH 494:\n",
      "LOSS train 0.11504461616277695 valid 0.11677007377147675\n",
      "EPOCH 495:\n",
      "LOSS train 0.120188407599926 valid 0.10999569296836853\n",
      "EPOCH 496:\n",
      "LOSS train 0.114426139742136 valid 0.1390419900417328\n",
      "EPOCH 497:\n",
      "LOSS train 0.12436345219612122 valid 0.12966613471508026\n",
      "EPOCH 498:\n",
      "LOSS train 0.13228251039981842 valid 0.11159767210483551\n",
      "EPOCH 499:\n",
      "LOSS train 0.1250808984041214 valid 0.12209095060825348\n",
      "EPOCH 500:\n",
      "LOSS train 0.12226816266775131 valid 0.1186741292476654\n",
      "EPOCH 501:\n",
      "LOSS train 0.12175537645816803 valid 0.12552931904792786\n",
      "EPOCH 502:\n",
      "LOSS train 0.13194815069437027 valid 0.13014951348304749\n",
      "EPOCH 503:\n",
      "LOSS train 0.11799565702676773 valid 0.1107729896903038\n",
      "EPOCH 504:\n",
      "LOSS train 0.12535623461008072 valid 0.12953272461891174\n",
      "EPOCH 505:\n",
      "LOSS train 0.12505611777305603 valid 0.11657711863517761\n",
      "EPOCH 506:\n",
      "LOSS train 0.12090866640210152 valid 0.14868219196796417\n",
      "EPOCH 507:\n",
      "LOSS train 0.1259540691971779 valid 0.14369098842144012\n",
      "EPOCH 508:\n",
      "LOSS train 0.11917739361524582 valid 0.1238497793674469\n",
      "EPOCH 509:\n",
      "LOSS train 0.11538911238312721 valid 0.11119473725557327\n",
      "EPOCH 510:\n",
      "LOSS train 0.1160985641181469 valid 0.11507700383663177\n",
      "EPOCH 511:\n",
      "LOSS train 0.11747917905449867 valid 0.1020825058221817\n",
      "EPOCH 512:\n",
      "LOSS train 0.12171652168035507 valid 0.11875826120376587\n",
      "EPOCH 513:\n",
      "LOSS train 0.12798284366726875 valid 0.10985513031482697\n",
      "EPOCH 514:\n",
      "LOSS train 0.11889099702239037 valid 0.12390447407960892\n",
      "EPOCH 515:\n",
      "LOSS train 0.12213077768683434 valid 0.11926926672458649\n",
      "EPOCH 516:\n",
      "LOSS train 0.12129025161266327 valid 0.12039615213871002\n",
      "EPOCH 517:\n",
      "LOSS train 0.11772936582565308 valid 0.12490253150463104\n",
      "EPOCH 518:\n",
      "LOSS train 0.1188616007566452 valid 0.10618685930967331\n",
      "EPOCH 519:\n",
      "LOSS train 0.11743443831801414 valid 0.11366227269172668\n",
      "EPOCH 520:\n",
      "LOSS train 0.11489563062787056 valid 0.0987526923418045\n",
      "EPOCH 521:\n",
      "LOSS train 0.11924219131469727 valid 0.11500082910060883\n",
      "EPOCH 522:\n",
      "LOSS train 0.10947353020310402 valid 0.11129380762577057\n",
      "EPOCH 523:\n",
      "LOSS train 0.11293474584817886 valid 0.09897410869598389\n",
      "EPOCH 524:\n",
      "LOSS train 0.11103474348783493 valid 0.1156543642282486\n",
      "EPOCH 525:\n",
      "LOSS train 0.1162925586104393 valid 0.10511574894189835\n",
      "EPOCH 526:\n",
      "LOSS train 0.11145764216780663 valid 0.09169413149356842\n",
      "EPOCH 527:\n",
      "LOSS train 0.12238752469420433 valid 0.12586407363414764\n",
      "EPOCH 528:\n",
      "LOSS train 0.11074435338377953 valid 0.11405804753303528\n",
      "EPOCH 529:\n",
      "LOSS train 0.11515704914927483 valid 0.09604838490486145\n",
      "EPOCH 530:\n",
      "LOSS train 0.1161811463534832 valid 0.12851867079734802\n",
      "EPOCH 531:\n",
      "LOSS train 0.12460227310657501 valid 0.11802692711353302\n",
      "EPOCH 532:\n",
      "LOSS train 0.12264411896467209 valid 0.10842657089233398\n",
      "EPOCH 533:\n",
      "LOSS train 0.10573158785700798 valid 0.10500926524400711\n",
      "EPOCH 534:\n",
      "LOSS train 0.11602599173784256 valid 0.12724944949150085\n",
      "EPOCH 535:\n",
      "LOSS train 0.1150076724588871 valid 0.13387858867645264\n",
      "EPOCH 536:\n",
      "LOSS train 0.11301242187619209 valid 0.12069046497344971\n",
      "EPOCH 537:\n",
      "LOSS train 0.11139436438679695 valid 0.09711672365665436\n",
      "EPOCH 538:\n",
      "LOSS train 0.1092911846935749 valid 0.1126316487789154\n",
      "EPOCH 539:\n",
      "LOSS train 0.11199332028627396 valid 0.12360449880361557\n",
      "EPOCH 540:\n",
      "LOSS train 0.11376581713557243 valid 0.11275974661111832\n",
      "EPOCH 541:\n",
      "LOSS train 0.1156359314918518 valid 0.11237501353025436\n",
      "EPOCH 542:\n",
      "LOSS train 0.12363790720701218 valid 0.11987045407295227\n",
      "EPOCH 543:\n",
      "LOSS train 0.11919358372688293 valid 0.10870711505413055\n",
      "EPOCH 544:\n",
      "LOSS train 0.11260517686605453 valid 0.12437315285205841\n",
      "EPOCH 545:\n",
      "LOSS train 0.1078454852104187 valid 0.1176770031452179\n",
      "EPOCH 546:\n",
      "LOSS train 0.10837721824645996 valid 0.11522138118743896\n",
      "EPOCH 547:\n",
      "LOSS train 0.11769769713282585 valid 0.11087561398744583\n",
      "EPOCH 548:\n",
      "LOSS train 0.11573728173971176 valid 0.13245517015457153\n",
      "EPOCH 549:\n",
      "LOSS train 0.11674123257398605 valid 0.1060996726155281\n",
      "EPOCH 550:\n",
      "LOSS train 0.10654032230377197 valid 0.11216221004724503\n",
      "EPOCH 551:\n",
      "LOSS train 0.11700338870286942 valid 0.11169903725385666\n",
      "EPOCH 552:\n",
      "LOSS train 0.10704750195145607 valid 0.10446450114250183\n",
      "EPOCH 553:\n",
      "LOSS train 0.11419661343097687 valid 0.11382002383470535\n",
      "EPOCH 554:\n",
      "LOSS train 0.11752717569470406 valid 0.10105770826339722\n",
      "EPOCH 555:\n",
      "LOSS train 0.10933345928788185 valid 0.09990265965461731\n",
      "EPOCH 556:\n",
      "LOSS train 0.11455868184566498 valid 0.12293104827404022\n",
      "EPOCH 557:\n",
      "LOSS train 0.11023294180631638 valid 0.11615044623613358\n",
      "EPOCH 558:\n",
      "LOSS train 0.1139872632920742 valid 0.12657468020915985\n",
      "EPOCH 559:\n",
      "LOSS train 0.10397301986813545 valid 0.13495299220085144\n",
      "EPOCH 560:\n",
      "LOSS train 0.11472629010677338 valid 0.11538267135620117\n",
      "EPOCH 561:\n",
      "LOSS train 0.11428593099117279 valid 0.12997373938560486\n",
      "EPOCH 562:\n",
      "LOSS train 0.10259060561656952 valid 0.10615094006061554\n",
      "EPOCH 563:\n",
      "LOSS train 0.11105266585946083 valid 0.09932875633239746\n",
      "EPOCH 564:\n",
      "LOSS train 0.1053839698433876 valid 0.098873570561409\n",
      "EPOCH 565:\n",
      "LOSS train 0.11483586579561234 valid 0.12498988211154938\n",
      "EPOCH 566:\n",
      "LOSS train 0.11639231815934181 valid 0.12637969851493835\n",
      "EPOCH 567:\n",
      "LOSS train 0.11366355791687965 valid 0.14399965107440948\n",
      "EPOCH 568:\n",
      "LOSS train 0.10998060554265976 valid 0.10518830269575119\n",
      "EPOCH 569:\n",
      "LOSS train 0.11185089871287346 valid 0.1272338479757309\n",
      "EPOCH 570:\n",
      "LOSS train 0.11156826093792915 valid 0.10675585269927979\n",
      "EPOCH 571:\n",
      "LOSS train 0.11316822841763496 valid 0.10658378899097443\n",
      "EPOCH 572:\n",
      "LOSS train 0.10859134420752525 valid 0.13470706343650818\n",
      "EPOCH 573:\n",
      "LOSS train 0.10910632833838463 valid 0.10545289516448975\n",
      "EPOCH 574:\n",
      "LOSS train 0.10965598747134209 valid 0.11766939610242844\n",
      "EPOCH 575:\n",
      "LOSS train 0.10848662629723549 valid 0.11687103658914566\n",
      "EPOCH 576:\n",
      "LOSS train 0.09982979670166969 valid 0.09327129274606705\n",
      "EPOCH 577:\n",
      "LOSS train 0.10315000265836716 valid 0.10143087804317474\n",
      "EPOCH 578:\n",
      "LOSS train 0.10335037484765053 valid 0.09686516970396042\n",
      "EPOCH 579:\n",
      "LOSS train 0.11120425164699554 valid 0.12750008702278137\n",
      "EPOCH 580:\n",
      "LOSS train 0.1099877879023552 valid 0.09345593303442001\n",
      "EPOCH 581:\n",
      "LOSS train 0.10480334609746933 valid 0.12074296176433563\n",
      "EPOCH 582:\n",
      "LOSS train 0.10790487378835678 valid 0.11876420676708221\n",
      "EPOCH 583:\n",
      "LOSS train 0.11350706964731216 valid 0.130769282579422\n",
      "EPOCH 584:\n",
      "LOSS train 0.11416216194629669 valid 0.10938847064971924\n",
      "EPOCH 585:\n",
      "LOSS train 0.10759585350751877 valid 0.1015782505273819\n",
      "EPOCH 586:\n",
      "LOSS train 0.11100482940673828 valid 0.11741121113300323\n",
      "EPOCH 587:\n",
      "LOSS train 0.09543871879577637 valid 0.10096679627895355\n",
      "EPOCH 588:\n",
      "LOSS train 0.10876747965812683 valid 0.1051575243473053\n",
      "EPOCH 589:\n",
      "LOSS train 0.10502543300390244 valid 0.10958020389080048\n",
      "EPOCH 590:\n",
      "LOSS train 0.10513347014784813 valid 0.09494064003229141\n",
      "EPOCH 591:\n",
      "LOSS train 0.11281323805451393 valid 0.12800267338752747\n",
      "EPOCH 592:\n",
      "LOSS train 0.11740570515394211 valid 0.12469178438186646\n",
      "EPOCH 593:\n",
      "LOSS train 0.11484935507178307 valid 0.11215099692344666\n",
      "EPOCH 594:\n",
      "LOSS train 0.11049062013626099 valid 0.13098162412643433\n",
      "EPOCH 595:\n",
      "LOSS train 0.10343831032514572 valid 0.10562261939048767\n",
      "EPOCH 596:\n",
      "LOSS train 0.10804238170385361 valid 0.09345534443855286\n",
      "EPOCH 597:\n",
      "LOSS train 0.11315371096134186 valid 0.1198766678571701\n",
      "EPOCH 598:\n",
      "LOSS train 0.10550497472286224 valid 0.10378081351518631\n",
      "EPOCH 599:\n",
      "LOSS train 0.10664048418402672 valid 0.11449995636940002\n",
      "EPOCH 600:\n",
      "LOSS train 0.09992177039384842 valid 0.1089182049036026\n",
      "EPOCH 601:\n",
      "LOSS train 0.10421115905046463 valid 0.11417858302593231\n",
      "EPOCH 602:\n",
      "LOSS train 0.09894630312919617 valid 0.09505712240934372\n",
      "EPOCH 603:\n",
      "LOSS train 0.10805360227823257 valid 0.11612921208143234\n",
      "EPOCH 604:\n",
      "LOSS train 0.10555524006485939 valid 0.1181245744228363\n",
      "EPOCH 605:\n",
      "LOSS train 0.09436307847499847 valid 0.11989790201187134\n",
      "EPOCH 606:\n",
      "LOSS train 0.10575558245182037 valid 0.09593260288238525\n",
      "EPOCH 607:\n",
      "LOSS train 0.10281339660286903 valid 0.09020161628723145\n",
      "EPOCH 608:\n",
      "LOSS train 0.10868309438228607 valid 0.09904977679252625\n",
      "EPOCH 609:\n",
      "LOSS train 0.10011570528149605 valid 0.1157895103096962\n",
      "EPOCH 610:\n",
      "LOSS train 0.11086239665746689 valid 0.11689069867134094\n",
      "EPOCH 611:\n",
      "LOSS train 0.10450810939073563 valid 0.08691219240427017\n",
      "EPOCH 612:\n",
      "LOSS train 0.09927750751376152 valid 0.09792192280292511\n",
      "EPOCH 613:\n",
      "LOSS train 0.10215899348258972 valid 0.11919844895601273\n",
      "EPOCH 614:\n",
      "LOSS train 0.10889949277043343 valid 0.10020565986633301\n",
      "EPOCH 615:\n",
      "LOSS train 0.10885247588157654 valid 0.10665861517190933\n",
      "EPOCH 616:\n",
      "LOSS train 0.1071256548166275 valid 0.11354482173919678\n",
      "EPOCH 617:\n",
      "LOSS train 0.10918894037604332 valid 0.10990612208843231\n",
      "EPOCH 618:\n",
      "LOSS train 0.0971546433866024 valid 0.10532078146934509\n",
      "EPOCH 619:\n",
      "LOSS train 0.10691312700510025 valid 0.09558702260255814\n",
      "EPOCH 620:\n",
      "LOSS train 0.1026787981390953 valid 0.1110086441040039\n",
      "EPOCH 621:\n",
      "LOSS train 0.1094597764313221 valid 0.1127881407737732\n",
      "EPOCH 622:\n",
      "LOSS train 0.11151992529630661 valid 0.10747872292995453\n",
      "EPOCH 623:\n",
      "LOSS train 0.1092403307557106 valid 0.10811406373977661\n",
      "EPOCH 624:\n",
      "LOSS train 0.10495971143245697 valid 0.09302471578121185\n",
      "EPOCH 625:\n",
      "LOSS train 0.1058051697909832 valid 0.09671875834465027\n",
      "EPOCH 626:\n",
      "LOSS train 0.0945621207356453 valid 0.102309949696064\n",
      "EPOCH 627:\n",
      "LOSS train 0.10202270746231079 valid 0.0868116095662117\n",
      "EPOCH 628:\n",
      "LOSS train 0.10114394500851631 valid 0.09826607257127762\n",
      "EPOCH 629:\n",
      "LOSS train 0.10251491144299507 valid 0.11470268666744232\n",
      "EPOCH 630:\n",
      "LOSS train 0.10323506966233253 valid 0.09809201955795288\n",
      "EPOCH 631:\n",
      "LOSS train 0.09656867012381554 valid 0.11468866467475891\n",
      "EPOCH 632:\n",
      "LOSS train 0.10919627547264099 valid 0.10288968682289124\n",
      "EPOCH 633:\n",
      "LOSS train 0.0944959968328476 valid 0.10164061188697815\n",
      "EPOCH 634:\n",
      "LOSS train 0.09526928886771202 valid 0.1243542730808258\n",
      "EPOCH 635:\n",
      "LOSS train 0.09918780252337456 valid 0.09790798276662827\n",
      "EPOCH 636:\n",
      "LOSS train 0.1011224314570427 valid 0.11455544829368591\n",
      "EPOCH 637:\n",
      "LOSS train 0.10401520505547523 valid 0.08982376009225845\n",
      "EPOCH 638:\n",
      "LOSS train 0.0962558165192604 valid 0.10983435809612274\n",
      "EPOCH 639:\n",
      "LOSS train 0.10577313601970673 valid 0.092673160135746\n",
      "EPOCH 640:\n",
      "LOSS train 0.10263340175151825 valid 0.10027922689914703\n",
      "EPOCH 641:\n",
      "LOSS train 0.10468722879886627 valid 0.1074526309967041\n",
      "EPOCH 642:\n",
      "LOSS train 0.10108546167612076 valid 0.1186823695898056\n",
      "EPOCH 643:\n",
      "LOSS train 0.102399080991745 valid 0.1255187690258026\n",
      "EPOCH 644:\n",
      "LOSS train 0.09899359941482544 valid 0.08575554192066193\n",
      "EPOCH 645:\n",
      "LOSS train 0.09812077134847641 valid 0.104853555560112\n",
      "EPOCH 646:\n",
      "LOSS train 0.0917709730565548 valid 0.09770925343036652\n",
      "EPOCH 647:\n",
      "LOSS train 0.09760092571377754 valid 0.09980088472366333\n",
      "EPOCH 648:\n",
      "LOSS train 0.09861651435494423 valid 0.09819139540195465\n",
      "EPOCH 649:\n",
      "LOSS train 0.09517206624150276 valid 0.12567520141601562\n",
      "EPOCH 650:\n",
      "LOSS train 0.09822575002908707 valid 0.09913184493780136\n",
      "EPOCH 651:\n",
      "LOSS train 0.09459226951003075 valid 0.0999024286866188\n",
      "EPOCH 652:\n",
      "LOSS train 0.0861288569867611 valid 0.10156239569187164\n",
      "EPOCH 653:\n",
      "LOSS train 0.10011866316199303 valid 0.12511779367923737\n",
      "EPOCH 654:\n",
      "LOSS train 0.09545508027076721 valid 0.0900709331035614\n",
      "EPOCH 655:\n",
      "LOSS train 0.09566574543714523 valid 0.09693960100412369\n",
      "EPOCH 656:\n",
      "LOSS train 0.09043968468904495 valid 0.10661797225475311\n",
      "EPOCH 657:\n",
      "LOSS train 0.09191471338272095 valid 0.09615342319011688\n",
      "EPOCH 658:\n",
      "LOSS train 0.10261743888258934 valid 0.09212270379066467\n",
      "EPOCH 659:\n",
      "LOSS train 0.09585720300674438 valid 0.10344590246677399\n",
      "EPOCH 660:\n",
      "LOSS train 0.0970756895840168 valid 0.09919683635234833\n",
      "EPOCH 661:\n",
      "LOSS train 0.10316475480794907 valid 0.09601271152496338\n",
      "EPOCH 662:\n",
      "LOSS train 0.0999990776181221 valid 0.09600189328193665\n",
      "EPOCH 663:\n",
      "LOSS train 0.09454847127199173 valid 0.12510952353477478\n",
      "EPOCH 664:\n",
      "LOSS train 0.09471886232495308 valid 0.10642126202583313\n",
      "EPOCH 665:\n",
      "LOSS train 0.10060933232307434 valid 0.11123078316450119\n",
      "EPOCH 666:\n",
      "LOSS train 0.09809434786438942 valid 0.10294599831104279\n",
      "EPOCH 667:\n",
      "LOSS train 0.09446470066905022 valid 0.10240985453128815\n",
      "EPOCH 668:\n",
      "LOSS train 0.09392957761883736 valid 0.09072883427143097\n",
      "EPOCH 669:\n",
      "LOSS train 0.08748475089669228 valid 0.09444206207990646\n",
      "EPOCH 670:\n",
      "LOSS train 0.10225189477205276 valid 0.08617118746042252\n",
      "EPOCH 671:\n",
      "LOSS train 0.0975869856774807 valid 0.10188953578472137\n",
      "EPOCH 672:\n",
      "LOSS train 0.09939070045948029 valid 0.08743920922279358\n",
      "EPOCH 673:\n",
      "LOSS train 0.09714354947209358 valid 0.09921202063560486\n",
      "EPOCH 674:\n",
      "LOSS train 0.09582927823066711 valid 0.10208175331354141\n",
      "EPOCH 675:\n",
      "LOSS train 0.09913183376193047 valid 0.11835917085409164\n",
      "EPOCH 676:\n",
      "LOSS train 0.09293879196047783 valid 0.09959720075130463\n",
      "EPOCH 677:\n",
      "LOSS train 0.09339749068021774 valid 0.08443894982337952\n",
      "EPOCH 678:\n",
      "LOSS train 0.09938567504286766 valid 0.11245604604482651\n",
      "EPOCH 679:\n",
      "LOSS train 0.09609990566968918 valid 0.10334648191928864\n",
      "EPOCH 680:\n",
      "LOSS train 0.09680193290114403 valid 0.0857049971818924\n",
      "EPOCH 681:\n",
      "LOSS train 0.09284390136599541 valid 0.08809132874011993\n",
      "EPOCH 682:\n",
      "LOSS train 0.09001696109771729 valid 0.08763270080089569\n",
      "EPOCH 683:\n",
      "LOSS train 0.09690213575959206 valid 0.08472046256065369\n",
      "EPOCH 684:\n",
      "LOSS train 0.09613854065537453 valid 0.09489605575799942\n",
      "EPOCH 685:\n",
      "LOSS train 0.09370242059230804 valid 0.11913527548313141\n",
      "EPOCH 686:\n",
      "LOSS train 0.09731146693229675 valid 0.09840881824493408\n",
      "EPOCH 687:\n",
      "LOSS train 0.08279363438487053 valid 0.0978052169084549\n",
      "EPOCH 688:\n",
      "LOSS train 0.08796228468418121 valid 0.08846590667963028\n",
      "EPOCH 689:\n",
      "LOSS train 0.09567710757255554 valid 0.1081727147102356\n",
      "EPOCH 690:\n",
      "LOSS train 0.08884350582957268 valid 0.09631773829460144\n",
      "EPOCH 691:\n",
      "LOSS train 0.0917390026152134 valid 0.10002590715885162\n",
      "EPOCH 692:\n",
      "LOSS train 0.09503521770238876 valid 0.0814443975687027\n",
      "EPOCH 693:\n",
      "LOSS train 0.08404799550771713 valid 0.10487323999404907\n",
      "EPOCH 694:\n",
      "LOSS train 0.09239602833986282 valid 0.10342414677143097\n",
      "EPOCH 695:\n",
      "LOSS train 0.1000158041715622 valid 0.10195140540599823\n",
      "EPOCH 696:\n",
      "LOSS train 0.08980505913496017 valid 0.11338748782873154\n",
      "EPOCH 697:\n",
      "LOSS train 0.09609277918934822 valid 0.09388040006160736\n",
      "EPOCH 698:\n",
      "LOSS train 0.085579514503479 valid 0.10636070370674133\n",
      "EPOCH 699:\n",
      "LOSS train 0.0868108943104744 valid 0.10693570971488953\n",
      "EPOCH 700:\n",
      "LOSS train 0.0892760381102562 valid 0.09635761380195618\n",
      "EPOCH 701:\n",
      "LOSS train 0.09821868315339088 valid 0.11432882398366928\n",
      "EPOCH 702:\n",
      "LOSS train 0.09205784648656845 valid 0.09856252372264862\n",
      "EPOCH 703:\n",
      "LOSS train 0.09181120619177818 valid 0.08471957594156265\n",
      "EPOCH 704:\n",
      "LOSS train 0.08758712559938431 valid 0.1032845750451088\n",
      "EPOCH 705:\n",
      "LOSS train 0.09369074925780296 valid 0.08832044899463654\n",
      "EPOCH 706:\n",
      "LOSS train 0.08212396129965782 valid 0.10529986023902893\n",
      "EPOCH 707:\n",
      "LOSS train 0.08782834559679031 valid 0.09956499934196472\n",
      "EPOCH 708:\n",
      "LOSS train 0.0925760306417942 valid 0.08538796007633209\n",
      "EPOCH 709:\n",
      "LOSS train 0.0888788215816021 valid 0.08730290085077286\n",
      "EPOCH 710:\n",
      "LOSS train 0.0879528671503067 valid 0.09356977045536041\n",
      "EPOCH 711:\n",
      "LOSS train 0.08917168155312538 valid 0.09990796446800232\n",
      "EPOCH 712:\n",
      "LOSS train 0.08569633215665817 valid 0.08415523171424866\n",
      "EPOCH 713:\n",
      "LOSS train 0.0927714928984642 valid 0.08554679155349731\n",
      "EPOCH 714:\n",
      "LOSS train 0.0927376076579094 valid 0.08915389329195023\n",
      "EPOCH 715:\n",
      "LOSS train 0.08860800042748451 valid 0.08241559565067291\n",
      "EPOCH 716:\n",
      "LOSS train 0.08732637763023376 valid 0.09526636451482773\n",
      "EPOCH 717:\n",
      "LOSS train 0.08688466995954514 valid 0.09338074177503586\n",
      "EPOCH 718:\n",
      "LOSS train 0.08952337503433228 valid 0.08501404523849487\n",
      "EPOCH 719:\n",
      "LOSS train 0.08961701020598412 valid 0.08729509264230728\n",
      "EPOCH 720:\n",
      "LOSS train 0.0879749245941639 valid 0.08220435678958893\n",
      "EPOCH 721:\n",
      "LOSS train 0.08542507141828537 valid 0.09470337629318237\n",
      "EPOCH 722:\n",
      "LOSS train 0.08917177841067314 valid 0.08694932609796524\n",
      "EPOCH 723:\n",
      "LOSS train 0.09388483688235283 valid 0.08879974484443665\n",
      "EPOCH 724:\n",
      "LOSS train 0.08638980239629745 valid 0.11287818104028702\n",
      "EPOCH 725:\n",
      "LOSS train 0.09819219261407852 valid 0.0973297506570816\n",
      "EPOCH 726:\n",
      "LOSS train 0.08718504011631012 valid 0.07461978495121002\n",
      "EPOCH 727:\n",
      "LOSS train 0.0874539241194725 valid 0.08616743981838226\n",
      "EPOCH 728:\n",
      "LOSS train 0.09011483192443848 valid 0.08964412659406662\n",
      "EPOCH 729:\n",
      "LOSS train 0.0890066958963871 valid 0.09091084450483322\n",
      "EPOCH 730:\n",
      "LOSS train 0.08773109316825867 valid 0.08852803707122803\n",
      "EPOCH 731:\n",
      "LOSS train 0.08772139996290207 valid 0.08249612897634506\n",
      "EPOCH 732:\n",
      "LOSS train 0.08537039160728455 valid 0.10156060010194778\n",
      "EPOCH 733:\n",
      "LOSS train 0.0808170735836029 valid 0.08218149840831757\n",
      "EPOCH 734:\n",
      "LOSS train 0.08024812117218971 valid 0.08978479355573654\n",
      "EPOCH 735:\n",
      "LOSS train 0.08949178457260132 valid 0.0993884801864624\n",
      "EPOCH 736:\n",
      "LOSS train 0.08281419798731804 valid 0.07435071468353271\n",
      "EPOCH 737:\n",
      "LOSS train 0.08762399479746819 valid 0.09602546691894531\n",
      "EPOCH 738:\n",
      "LOSS train 0.08309700340032578 valid 0.08347462117671967\n",
      "EPOCH 739:\n",
      "LOSS train 0.08370477333664894 valid 0.08303268253803253\n",
      "EPOCH 740:\n",
      "LOSS train 0.08520493656396866 valid 0.10108429193496704\n",
      "EPOCH 741:\n",
      "LOSS train 0.07919712364673615 valid 0.08510136604309082\n",
      "EPOCH 742:\n",
      "LOSS train 0.08364921808242798 valid 0.09705638140439987\n",
      "EPOCH 743:\n",
      "LOSS train 0.08884525299072266 valid 0.10109147429466248\n",
      "EPOCH 744:\n",
      "LOSS train 0.08882289007306099 valid 0.0868021696805954\n",
      "EPOCH 745:\n",
      "LOSS train 0.0858631506562233 valid 0.10120329260826111\n",
      "EPOCH 746:\n",
      "LOSS train 0.09034813195466995 valid 0.08735068142414093\n",
      "EPOCH 747:\n",
      "LOSS train 0.09285902604460716 valid 0.09568993747234344\n",
      "EPOCH 748:\n",
      "LOSS train 0.07822031155228615 valid 0.07709742337465286\n",
      "EPOCH 749:\n",
      "LOSS train 0.07974320650100708 valid 0.09751349687576294\n",
      "EPOCH 750:\n",
      "LOSS train 0.08735275268554688 valid 0.07411348819732666\n",
      "EPOCH 751:\n",
      "LOSS train 0.07948620244860649 valid 0.09533143043518066\n",
      "EPOCH 752:\n",
      "LOSS train 0.08099905028939247 valid 0.07868024706840515\n",
      "EPOCH 753:\n",
      "LOSS train 0.08911435306072235 valid 0.08107718080282211\n",
      "EPOCH 754:\n",
      "LOSS train 0.08118626475334167 valid 0.0894089937210083\n",
      "EPOCH 755:\n",
      "LOSS train 0.08905029296875 valid 0.09051874279975891\n",
      "EPOCH 756:\n",
      "LOSS train 0.08254772424697876 valid 0.0979008823633194\n",
      "EPOCH 757:\n",
      "LOSS train 0.08528076484799385 valid 0.08129440248012543\n",
      "EPOCH 758:\n",
      "LOSS train 0.0868414118885994 valid 0.07167281210422516\n",
      "EPOCH 759:\n",
      "LOSS train 0.07956256344914436 valid 0.08179357647895813\n",
      "EPOCH 760:\n",
      "LOSS train 0.08106748759746552 valid 0.09839193522930145\n",
      "EPOCH 761:\n",
      "LOSS train 0.0760275237262249 valid 0.11362367868423462\n",
      "EPOCH 762:\n",
      "LOSS train 0.08546330407261848 valid 0.08823320269584656\n",
      "EPOCH 763:\n",
      "LOSS train 0.08952784538269043 valid 0.08825277537107468\n",
      "EPOCH 764:\n",
      "LOSS train 0.0900963693857193 valid 0.08292806893587112\n",
      "EPOCH 765:\n",
      "LOSS train 0.08178797364234924 valid 0.08105185627937317\n",
      "EPOCH 766:\n",
      "LOSS train 0.08663342893123627 valid 0.08457411825656891\n",
      "EPOCH 767:\n",
      "LOSS train 0.08203862607479095 valid 0.08670194447040558\n",
      "EPOCH 768:\n",
      "LOSS train 0.08207528665661812 valid 0.10236299782991409\n",
      "EPOCH 769:\n",
      "LOSS train 0.08490940742194653 valid 0.07283658534288406\n",
      "EPOCH 770:\n",
      "LOSS train 0.08485234156250954 valid 0.08278775215148926\n",
      "EPOCH 771:\n",
      "LOSS train 0.0777660422027111 valid 0.0773167833685875\n",
      "EPOCH 772:\n",
      "LOSS train 0.08457130566239357 valid 0.07359758764505386\n",
      "EPOCH 773:\n",
      "LOSS train 0.085525743663311 valid 0.09843770414590836\n",
      "EPOCH 774:\n",
      "LOSS train 0.08404397591948509 valid 0.09221456944942474\n",
      "EPOCH 775:\n",
      "LOSS train 0.08492090180516243 valid 0.09110474586486816\n",
      "EPOCH 776:\n",
      "LOSS train 0.08068821020424366 valid 0.12111679464578629\n",
      "EPOCH 777:\n",
      "LOSS train 0.07815457135438919 valid 0.08600768446922302\n",
      "EPOCH 778:\n",
      "LOSS train 0.0820319801568985 valid 0.09202506393194199\n",
      "EPOCH 779:\n",
      "LOSS train 0.08560170605778694 valid 0.0963561087846756\n",
      "EPOCH 780:\n",
      "LOSS train 0.08873482421040535 valid 0.08342009782791138\n",
      "EPOCH 781:\n",
      "LOSS train 0.08098709210753441 valid 0.07216826826334\n",
      "EPOCH 782:\n",
      "LOSS train 0.08272507414221764 valid 0.09177528321743011\n",
      "EPOCH 783:\n",
      "LOSS train 0.07939179241657257 valid 0.09560912847518921\n",
      "EPOCH 784:\n",
      "LOSS train 0.07521285489201546 valid 0.08034554123878479\n",
      "EPOCH 785:\n",
      "LOSS train 0.08104192093014717 valid 0.08544595539569855\n",
      "EPOCH 786:\n",
      "LOSS train 0.07438507676124573 valid 0.0875970721244812\n",
      "EPOCH 787:\n",
      "LOSS train 0.07616997510194778 valid 0.08427807688713074\n",
      "EPOCH 788:\n",
      "LOSS train 0.07692473381757736 valid 0.08516714721918106\n",
      "EPOCH 789:\n",
      "LOSS train 0.08274206519126892 valid 0.08502605557441711\n",
      "EPOCH 790:\n",
      "LOSS train 0.07907970994710922 valid 0.07698674499988556\n",
      "EPOCH 791:\n",
      "LOSS train 0.072323277592659 valid 0.06976735591888428\n",
      "EPOCH 792:\n",
      "LOSS train 0.08673267066478729 valid 0.09055130183696747\n",
      "EPOCH 793:\n",
      "LOSS train 0.07586855441331863 valid 0.07481557130813599\n",
      "EPOCH 794:\n",
      "LOSS train 0.0746137946844101 valid 0.0926099568605423\n",
      "EPOCH 795:\n",
      "LOSS train 0.07009188458323479 valid 0.08626510947942734\n",
      "EPOCH 796:\n",
      "LOSS train 0.07739610597491264 valid 0.10549120604991913\n",
      "EPOCH 797:\n",
      "LOSS train 0.07387186214327812 valid 0.0845719501376152\n",
      "EPOCH 798:\n",
      "LOSS train 0.08154278248548508 valid 0.09849502146244049\n",
      "EPOCH 799:\n",
      "LOSS train 0.08403553813695908 valid 0.08047173917293549\n",
      "EPOCH 800:\n",
      "LOSS train 0.07670285180211067 valid 0.08905851095914841\n",
      "EPOCH 801:\n",
      "LOSS train 0.08292835205793381 valid 0.07069131731987\n",
      "EPOCH 802:\n",
      "LOSS train 0.07395995035767555 valid 0.08205170929431915\n",
      "EPOCH 803:\n",
      "LOSS train 0.07741120457649231 valid 0.08168719708919525\n",
      "EPOCH 804:\n",
      "LOSS train 0.08133772015571594 valid 0.07180939614772797\n",
      "EPOCH 805:\n",
      "LOSS train 0.07504261285066605 valid 0.08411145210266113\n",
      "EPOCH 806:\n",
      "LOSS train 0.07641955837607384 valid 0.0857677161693573\n",
      "EPOCH 807:\n",
      "LOSS train 0.07447485253214836 valid 0.09315434098243713\n",
      "EPOCH 808:\n",
      "LOSS train 0.08304588496685028 valid 0.07578536868095398\n",
      "EPOCH 809:\n",
      "LOSS train 0.07961197569966316 valid 0.06353931128978729\n",
      "EPOCH 810:\n",
      "LOSS train 0.07379273697733879 valid 0.09898175299167633\n",
      "EPOCH 811:\n",
      "LOSS train 0.07363585755228996 valid 0.10382804274559021\n",
      "EPOCH 812:\n",
      "LOSS train 0.07222462445497513 valid 0.09279511868953705\n",
      "EPOCH 813:\n",
      "LOSS train 0.0761086456477642 valid 0.07422491908073425\n",
      "EPOCH 814:\n",
      "LOSS train 0.07452195510268211 valid 0.11653251945972443\n",
      "EPOCH 815:\n",
      "LOSS train 0.07919896766543388 valid 0.07227417826652527\n",
      "EPOCH 816:\n",
      "LOSS train 0.07939629256725311 valid 0.08696851879358292\n",
      "EPOCH 817:\n",
      "LOSS train 0.0696098692715168 valid 0.09502137452363968\n",
      "EPOCH 818:\n",
      "LOSS train 0.07628053799271584 valid 0.08070400357246399\n",
      "EPOCH 819:\n",
      "LOSS train 0.0770227387547493 valid 0.11179813742637634\n",
      "EPOCH 820:\n",
      "LOSS train 0.06975062564015388 valid 0.09980762749910355\n",
      "EPOCH 821:\n",
      "LOSS train 0.07267379760742188 valid 0.08525742590427399\n",
      "EPOCH 822:\n",
      "LOSS train 0.07875833101570606 valid 0.0921522006392479\n",
      "EPOCH 823:\n",
      "LOSS train 0.07911165058612823 valid 0.08686693012714386\n",
      "EPOCH 824:\n",
      "LOSS train 0.08155503123998642 valid 0.07149014621973038\n",
      "EPOCH 825:\n",
      "LOSS train 0.06758319959044456 valid 0.08213020116090775\n",
      "EPOCH 826:\n",
      "LOSS train 0.07331093773245811 valid 0.09203913807868958\n",
      "EPOCH 827:\n",
      "LOSS train 0.07469605654478073 valid 0.08516772091388702\n",
      "EPOCH 828:\n",
      "LOSS train 0.07075246796011925 valid 0.07923568040132523\n",
      "EPOCH 829:\n",
      "LOSS train 0.07645754143595695 valid 0.07513484358787537\n",
      "EPOCH 830:\n",
      "LOSS train 0.07646781206130981 valid 0.07035930454730988\n",
      "EPOCH 831:\n",
      "LOSS train 0.07809443771839142 valid 0.08710329234600067\n",
      "EPOCH 832:\n",
      "LOSS train 0.06683356314897537 valid 0.09203232079744339\n",
      "EPOCH 833:\n",
      "LOSS train 0.07784739881753922 valid 0.0792304128408432\n",
      "EPOCH 834:\n",
      "LOSS train 0.07865891605615616 valid 0.06008962541818619\n",
      "EPOCH 835:\n",
      "LOSS train 0.07011038064956665 valid 0.0718194991350174\n",
      "EPOCH 836:\n",
      "LOSS train 0.07425063848495483 valid 0.08579046279191971\n",
      "EPOCH 837:\n",
      "LOSS train 0.07498184591531754 valid 0.07238321751356125\n",
      "EPOCH 838:\n",
      "LOSS train 0.07957411929965019 valid 0.07269857823848724\n",
      "EPOCH 839:\n",
      "LOSS train 0.07512377202510834 valid 0.07528571784496307\n",
      "EPOCH 840:\n",
      "LOSS train 0.07351332902908325 valid 0.06988248974084854\n",
      "EPOCH 841:\n",
      "LOSS train 0.07798964902758598 valid 0.0789550319314003\n",
      "EPOCH 842:\n",
      "LOSS train 0.07785865291953087 valid 0.06829454004764557\n",
      "EPOCH 843:\n",
      "LOSS train 0.0708368793129921 valid 0.0959157943725586\n",
      "EPOCH 844:\n",
      "LOSS train 0.07298273593187332 valid 0.07730944454669952\n",
      "EPOCH 845:\n",
      "LOSS train 0.07833647727966309 valid 0.07775552570819855\n",
      "EPOCH 846:\n",
      "LOSS train 0.07067478448152542 valid 0.09511485695838928\n",
      "EPOCH 847:\n",
      "LOSS train 0.07990504801273346 valid 0.09721517562866211\n",
      "EPOCH 848:\n",
      "LOSS train 0.06466017290949821 valid 0.1038842722773552\n",
      "EPOCH 849:\n",
      "LOSS train 0.0743174534291029 valid 0.06902764737606049\n",
      "EPOCH 850:\n",
      "LOSS train 0.07437605410814285 valid 0.09431344270706177\n",
      "EPOCH 851:\n",
      "LOSS train 0.06501318141818047 valid 0.06915470957756042\n",
      "EPOCH 852:\n",
      "LOSS train 0.07291670888662338 valid 0.08286648988723755\n",
      "EPOCH 853:\n",
      "LOSS train 0.06982426159083843 valid 0.08793869614601135\n",
      "EPOCH 854:\n",
      "LOSS train 0.0796203464269638 valid 0.07925984263420105\n",
      "EPOCH 855:\n",
      "LOSS train 0.0738014280796051 valid 0.07654587924480438\n",
      "EPOCH 856:\n",
      "LOSS train 0.07544264197349548 valid 0.0760144591331482\n",
      "EPOCH 857:\n",
      "LOSS train 0.0685146227478981 valid 0.06580250710248947\n",
      "EPOCH 858:\n",
      "LOSS train 0.07391704991459846 valid 0.057984814047813416\n",
      "EPOCH 859:\n",
      "LOSS train 0.07334302365779877 valid 0.07378840446472168\n",
      "EPOCH 860:\n",
      "LOSS train 0.06773005425930023 valid 0.0864666998386383\n",
      "EPOCH 861:\n",
      "LOSS train 0.06798163801431656 valid 0.07226857542991638\n",
      "EPOCH 862:\n",
      "LOSS train 0.07621992006897926 valid 0.09317553043365479\n",
      "EPOCH 863:\n",
      "LOSS train 0.07164336368441582 valid 0.08518858999013901\n",
      "EPOCH 864:\n",
      "LOSS train 0.06933704018592834 valid 0.08186671137809753\n",
      "EPOCH 865:\n",
      "LOSS train 0.07596763595938683 valid 0.07453936338424683\n",
      "EPOCH 866:\n",
      "LOSS train 0.06919107586145401 valid 0.06727388501167297\n",
      "EPOCH 867:\n",
      "LOSS train 0.0705573558807373 valid 0.06322560459375381\n",
      "EPOCH 868:\n",
      "LOSS train 0.06990046054124832 valid 0.07827609777450562\n",
      "EPOCH 869:\n",
      "LOSS train 0.06686335057020187 valid 0.07010020315647125\n",
      "EPOCH 870:\n",
      "LOSS train 0.06992541253566742 valid 0.0927034541964531\n",
      "EPOCH 871:\n",
      "LOSS train 0.07226792350411415 valid 0.08940073847770691\n",
      "EPOCH 872:\n",
      "LOSS train 0.07297731190919876 valid 0.07605105638504028\n",
      "EPOCH 873:\n",
      "LOSS train 0.0635417029261589 valid 0.08564531803131104\n",
      "EPOCH 874:\n",
      "LOSS train 0.06177283823490143 valid 0.08363187313079834\n",
      "EPOCH 875:\n",
      "LOSS train 0.06586576625704765 valid 0.07602906227111816\n",
      "EPOCH 876:\n",
      "LOSS train 0.06493022665381432 valid 0.08828501403331757\n",
      "EPOCH 877:\n",
      "LOSS train 0.06955966725945473 valid 0.08661099523305893\n",
      "EPOCH 878:\n",
      "LOSS train 0.06993522495031357 valid 0.06776060909032822\n",
      "EPOCH 879:\n",
      "LOSS train 0.07129862159490585 valid 0.08413928747177124\n",
      "EPOCH 880:\n",
      "LOSS train 0.071015864610672 valid 0.07910404354333878\n",
      "EPOCH 881:\n",
      "LOSS train 0.0688384547829628 valid 0.06891182065010071\n",
      "EPOCH 882:\n",
      "LOSS train 0.0681394636631012 valid 0.06212400645017624\n",
      "EPOCH 883:\n",
      "LOSS train 0.07018687203526497 valid 0.06572829186916351\n",
      "EPOCH 884:\n",
      "LOSS train 0.0680186003446579 valid 0.059823695570230484\n",
      "EPOCH 885:\n",
      "LOSS train 0.06854994595050812 valid 0.0718570351600647\n",
      "EPOCH 886:\n",
      "LOSS train 0.06684247776865959 valid 0.08249156922101974\n",
      "EPOCH 887:\n",
      "LOSS train 0.06372933089733124 valid 0.07907886058092117\n",
      "EPOCH 888:\n",
      "LOSS train 0.07315488159656525 valid 0.06788697093725204\n",
      "EPOCH 889:\n",
      "LOSS train 0.07270373404026031 valid 0.06913667917251587\n",
      "EPOCH 890:\n",
      "LOSS train 0.06981568038463593 valid 0.06801975518465042\n",
      "EPOCH 891:\n",
      "LOSS train 0.06931329146027565 valid 0.07730401307344437\n",
      "EPOCH 892:\n",
      "LOSS train 0.06317945569753647 valid 0.06515826284885406\n",
      "EPOCH 893:\n",
      "LOSS train 0.0675671435892582 valid 0.0852440819144249\n",
      "EPOCH 894:\n",
      "LOSS train 0.06887161359190941 valid 0.07368816435337067\n",
      "EPOCH 895:\n",
      "LOSS train 0.07149061560630798 valid 0.0882350504398346\n",
      "EPOCH 896:\n",
      "LOSS train 0.06600385159254074 valid 0.07952989637851715\n",
      "EPOCH 897:\n",
      "LOSS train 0.06740903481841087 valid 0.06548289954662323\n",
      "EPOCH 898:\n",
      "LOSS train 0.06546659767627716 valid 0.06590824574232101\n",
      "EPOCH 899:\n",
      "LOSS train 0.07262576371431351 valid 0.06960822641849518\n",
      "EPOCH 900:\n",
      "LOSS train 0.06372267007827759 valid 0.07504178583621979\n",
      "EPOCH 901:\n",
      "LOSS train 0.06466208212077618 valid 0.08062444627285004\n",
      "EPOCH 902:\n",
      "LOSS train 0.06650730036199093 valid 0.07056665420532227\n",
      "EPOCH 903:\n",
      "LOSS train 0.06828551180660725 valid 0.06808699667453766\n",
      "EPOCH 904:\n",
      "LOSS train 0.0679099578410387 valid 0.0733383446931839\n",
      "EPOCH 905:\n",
      "LOSS train 0.0673561841249466 valid 0.06632910668849945\n",
      "EPOCH 906:\n",
      "LOSS train 0.0698417816311121 valid 0.059640806168317795\n",
      "EPOCH 907:\n",
      "LOSS train 0.06386355310678482 valid 0.06320533156394958\n",
      "EPOCH 908:\n",
      "LOSS train 0.07541514560580254 valid 0.07176516950130463\n",
      "EPOCH 909:\n",
      "LOSS train 0.06211026385426521 valid 0.08450694382190704\n",
      "EPOCH 910:\n",
      "LOSS train 0.07157620415091515 valid 0.09859533607959747\n",
      "EPOCH 911:\n",
      "LOSS train 0.06555137038230896 valid 0.073701411485672\n",
      "EPOCH 912:\n",
      "LOSS train 0.07055142149329185 valid 0.07383637875318527\n",
      "EPOCH 913:\n",
      "LOSS train 0.07184620574116707 valid 0.07529983669519424\n",
      "EPOCH 914:\n",
      "LOSS train 0.06594724208116531 valid 0.07026305794715881\n",
      "EPOCH 915:\n",
      "LOSS train 0.061233995482325554 valid 0.0737188383936882\n",
      "EPOCH 916:\n",
      "LOSS train 0.06570740416646004 valid 0.060984570533037186\n",
      "EPOCH 917:\n",
      "LOSS train 0.07128608040511608 valid 0.08703485131263733\n",
      "EPOCH 918:\n",
      "LOSS train 0.06518612056970596 valid 0.06459459662437439\n",
      "EPOCH 919:\n",
      "LOSS train 0.0635487474501133 valid 0.05464315786957741\n",
      "EPOCH 920:\n",
      "LOSS train 0.06850461661815643 valid 0.07906553149223328\n",
      "EPOCH 921:\n",
      "LOSS train 0.06701395660638809 valid 0.07114125788211823\n",
      "EPOCH 922:\n",
      "LOSS train 0.062155742198228836 valid 0.08282487839460373\n",
      "EPOCH 923:\n",
      "LOSS train 0.06543615274131298 valid 0.06626871228218079\n",
      "EPOCH 924:\n",
      "LOSS train 0.06614867597818375 valid 0.057159774005413055\n",
      "EPOCH 925:\n",
      "LOSS train 0.06265708059072495 valid 0.08261974155902863\n",
      "EPOCH 926:\n",
      "LOSS train 0.06568166799843311 valid 0.05842994898557663\n",
      "EPOCH 927:\n",
      "LOSS train 0.06254643946886063 valid 0.07096795737743378\n",
      "EPOCH 928:\n",
      "LOSS train 0.0587347075343132 valid 0.07473693788051605\n",
      "EPOCH 929:\n",
      "LOSS train 0.05580425262451172 valid 0.07353885471820831\n",
      "EPOCH 930:\n",
      "LOSS train 0.06431758031249046 valid 0.06321437656879425\n",
      "EPOCH 931:\n",
      "LOSS train 0.06336143799126148 valid 0.07767944782972336\n",
      "EPOCH 932:\n",
      "LOSS train 0.0666164793074131 valid 0.06755997985601425\n",
      "EPOCH 933:\n",
      "LOSS train 0.06510256230831146 valid 0.06847408413887024\n",
      "EPOCH 934:\n",
      "LOSS train 0.06706692092120647 valid 0.06378956139087677\n",
      "EPOCH 935:\n",
      "LOSS train 0.07167104259133339 valid 0.0847921371459961\n",
      "EPOCH 936:\n",
      "LOSS train 0.06763266772031784 valid 0.06957719475030899\n",
      "EPOCH 937:\n",
      "LOSS train 0.06040053255856037 valid 0.06079874560236931\n",
      "EPOCH 938:\n",
      "LOSS train 0.06277802772819996 valid 0.06484787166118622\n",
      "EPOCH 939:\n",
      "LOSS train 0.06287174671888351 valid 0.06641030311584473\n",
      "EPOCH 940:\n",
      "LOSS train 0.06282994896173477 valid 0.06432850658893585\n",
      "EPOCH 941:\n",
      "LOSS train 0.06584066711366177 valid 0.07535181939601898\n",
      "EPOCH 942:\n",
      "LOSS train 0.06318138167262077 valid 0.07516796886920929\n",
      "EPOCH 943:\n",
      "LOSS train 0.06138088367879391 valid 0.0808035135269165\n",
      "EPOCH 944:\n",
      "LOSS train 0.06406880915164948 valid 0.07576212286949158\n",
      "EPOCH 945:\n",
      "LOSS train 0.06225812807679176 valid 0.07026407122612\n",
      "EPOCH 946:\n",
      "LOSS train 0.06277550756931305 valid 0.051395613700151443\n",
      "EPOCH 947:\n",
      "LOSS train 0.06611523032188416 valid 0.05298888310790062\n",
      "EPOCH 948:\n",
      "LOSS train 0.06648249924182892 valid 0.0696650892496109\n",
      "EPOCH 949:\n",
      "LOSS train 0.06268544495105743 valid 0.07617077231407166\n",
      "EPOCH 950:\n",
      "LOSS train 0.05130773037672043 valid 0.0745074599981308\n",
      "EPOCH 951:\n",
      "LOSS train 0.06470893695950508 valid 0.08403518795967102\n",
      "EPOCH 952:\n",
      "LOSS train 0.057862915098667145 valid 0.06781817972660065\n",
      "EPOCH 953:\n",
      "LOSS train 0.0627952292561531 valid 0.06738948076963425\n",
      "EPOCH 954:\n",
      "LOSS train 0.060709789395332336 valid 0.07340341806411743\n",
      "EPOCH 955:\n",
      "LOSS train 0.07032908126711845 valid 0.08005640655755997\n",
      "EPOCH 956:\n",
      "LOSS train 0.06791605055332184 valid 0.06278172880411148\n",
      "EPOCH 957:\n",
      "LOSS train 0.06272523663938046 valid 0.06974241882562637\n",
      "EPOCH 958:\n",
      "LOSS train 0.06392405368387699 valid 0.06525155156850815\n",
      "EPOCH 959:\n",
      "LOSS train 0.06242332421243191 valid 0.06902003288269043\n",
      "EPOCH 960:\n",
      "LOSS train 0.06185176223516464 valid 0.055730290710926056\n",
      "EPOCH 961:\n",
      "LOSS train 0.06055326573550701 valid 0.0694994106888771\n",
      "EPOCH 962:\n",
      "LOSS train 0.060882698744535446 valid 0.08869592845439911\n",
      "EPOCH 963:\n",
      "LOSS train 0.061651911586523056 valid 0.059455662965774536\n",
      "EPOCH 964:\n",
      "LOSS train 0.06619607284665108 valid 0.06651115417480469\n",
      "EPOCH 965:\n",
      "LOSS train 0.0669336598366499 valid 0.06696830689907074\n",
      "EPOCH 966:\n",
      "LOSS train 0.05913071893155575 valid 0.06508009135723114\n",
      "EPOCH 967:\n",
      "LOSS train 0.06069575995206833 valid 0.07311999797821045\n",
      "EPOCH 968:\n",
      "LOSS train 0.06458888202905655 valid 0.055544912815093994\n",
      "EPOCH 969:\n",
      "LOSS train 0.061121877282857895 valid 0.06706219911575317\n",
      "EPOCH 970:\n",
      "LOSS train 0.06204361468553543 valid 0.06797683238983154\n",
      "EPOCH 971:\n",
      "LOSS train 0.06274430081248283 valid 0.06357599049806595\n",
      "EPOCH 972:\n",
      "LOSS train 0.06411329098045826 valid 0.06867856532335281\n",
      "EPOCH 973:\n",
      "LOSS train 0.05729329399764538 valid 0.07556046545505524\n",
      "EPOCH 974:\n",
      "LOSS train 0.06040167808532715 valid 0.07164928317070007\n",
      "EPOCH 975:\n",
      "LOSS train 0.05647584795951843 valid 0.07375739514827728\n",
      "EPOCH 976:\n",
      "LOSS train 0.062791358679533 valid 0.07369410246610641\n",
      "EPOCH 977:\n",
      "LOSS train 0.062137212604284286 valid 0.08398649096488953\n",
      "EPOCH 978:\n",
      "LOSS train 0.0641048401594162 valid 0.06052807718515396\n",
      "EPOCH 979:\n",
      "LOSS train 0.05297614261507988 valid 0.07264389097690582\n",
      "EPOCH 980:\n",
      "LOSS train 0.05484669469296932 valid 0.061287716031074524\n",
      "EPOCH 981:\n",
      "LOSS train 0.05458162724971771 valid 0.09500625729560852\n",
      "EPOCH 982:\n",
      "LOSS train 0.06390292197465897 valid 0.06377053260803223\n",
      "EPOCH 983:\n",
      "LOSS train 0.05896214209496975 valid 0.07105869054794312\n",
      "EPOCH 984:\n",
      "LOSS train 0.05879285931587219 valid 0.07158912718296051\n",
      "EPOCH 985:\n",
      "LOSS train 0.061194173991680145 valid 0.05475626885890961\n",
      "EPOCH 986:\n",
      "LOSS train 0.05819832533597946 valid 0.06320562958717346\n",
      "EPOCH 987:\n",
      "LOSS train 0.05409699305891991 valid 0.06204918399453163\n",
      "EPOCH 988:\n",
      "LOSS train 0.06460326351225376 valid 0.0545588955283165\n",
      "EPOCH 989:\n",
      "LOSS train 0.061836061999201775 valid 0.06335364282131195\n",
      "EPOCH 990:\n",
      "LOSS train 0.06072594225406647 valid 0.05689675733447075\n",
      "EPOCH 991:\n",
      "LOSS train 0.05476088263094425 valid 0.08080480247735977\n",
      "EPOCH 992:\n",
      "LOSS train 0.061624299734830856 valid 0.055160775780677795\n",
      "EPOCH 993:\n",
      "LOSS train 0.060639720410108566 valid 0.06485375016927719\n",
      "EPOCH 994:\n",
      "LOSS train 0.049830956384539604 valid 0.06328889727592468\n",
      "EPOCH 995:\n",
      "LOSS train 0.05740119516849518 valid 0.07537198066711426\n",
      "EPOCH 996:\n",
      "LOSS train 0.048634935170412064 valid 0.05689604580402374\n",
      "EPOCH 997:\n",
      "LOSS train 0.05958118662238121 valid 0.07113555073738098\n",
      "EPOCH 998:\n",
      "LOSS train 0.05507944896817207 valid 0.06037077307701111\n",
      "EPOCH 999:\n",
      "LOSS train 0.052307166159152985 valid 0.06362517178058624\n",
      "EPOCH 1000:\n",
      "LOSS train 0.058040373027324677 valid 0.07404490560293198\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 1000\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "  # Make sure gradient tracking is on, and do a pass over the data\n",
    "  model.train(True)\n",
    "  avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "  # We don't need gradients on to do reporting\n",
    "  model.train(False)\n",
    "\n",
    "  running_vloss = 0.0\n",
    "  for i, (vinputs, vlabels) in enumerate(test_dataloader):\n",
    "    vinputs = [a.to(device) for a in vinputs]\n",
    "    vlabels = [a.to(device) for a in vlabels]\n",
    "    voutputs = model(*vinputs)\n",
    "    vloss = loss_fn(voutputs[0], vlabels[0])\n",
    "    running_vloss += vloss\n",
    "\n",
    "  avg_vloss = running_vloss / (i + 1)\n",
    "  print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "  # Log the running loss averaged per batch\n",
    "  # for both training and validation\n",
    "  writer.add_scalars('Training vs. Validation Loss',\n",
    "                  { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                  epoch_number + 1)\n",
    "  writer.flush()\n",
    "\n",
    "  # Track best performance, and save the model's state\n",
    "  if avg_vloss < best_vloss:\n",
    "    best_vloss = avg_vloss\n",
    "    model_path = 'models/model_{}_{}'.format(timestamp, epoch_number)\n",
    "    th.save(model.state_dict(), model_path)\n",
    "\n",
    "  epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0. 0.] [ 0.3418274  -0.25506592] [[1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "flatten() takes from 0 to 1 positional arguments but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[181], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m*\u001b[39mtrain_dataset[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m----> 3\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m*\u001b[39mmodel(\u001b[39m*\u001b[39;49m[a\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m*\u001b[39;49ma\u001b[39m.\u001b[39;49mshape) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m train_dataset[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m]]))\n\u001b[0;32m      4\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m*\u001b[39mtrain_dataset[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\vcanaa\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[155], line 24\u001b[0m, in \u001b[0;36mMyModel.forward\u001b[1;34m(self, dir, jump, vel, wall)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mdir\u001b[39m, jump, vel, wall):\n\u001b[0;32m     20\u001b[0m   \u001b[39m# x = self.flatten(wall)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m   \u001b[39m# x = th.cat([dir, jump, vel, x], dim=1)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m   \u001b[39m# x = F.tanh(self.linear3(x))\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflatten(wall)\n\u001b[0;32m     25\u001b[0m   x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mtanh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwall1(x))\n\u001b[0;32m     26\u001b[0m   x \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mcat([\u001b[39mdir\u001b[39m, jump, vel, x], dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vcanaa\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\vcanaa\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\flatten.py:46\u001b[0m, in \u001b[0;36mFlatten.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mflatten(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstart_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mend_dim)\n",
      "\u001b[1;31mTypeError\u001b[0m: flatten() takes from 0 to 1 positional arguments but 2 were given"
     ]
    }
   ],
   "source": [
    "print(*train_dataset[0][0])\n",
    "with th.no_grad():\n",
    "  print(*model(*[a.reshape(1, *a.shape) for a in train_dataset[0][0]]))\n",
    "  print(*train_dataset[0][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
