{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "print('device:', device)\n",
    "\n",
    "saved_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# from collections import OrderedDict\n",
    "\n",
    "# from typing import List\n",
    "\n",
    "# class MovementDataSet(Dataset):\n",
    "#   def __init__(self, data_path: str):\n",
    "#     self.length = -1\n",
    "#     def load_data(type: str):\n",
    "#       dir_path: str = os.path.join(data_path, type) # ex: data/inputs\n",
    "#       data = OrderedDict()\n",
    "#       for f in os.listdir(dir_path):\n",
    "#         t = th.tensor(np.load(os.path.join(dir_path, f)).astype(np.float32))\n",
    "#         if self.length != -1 and len(t) != self.length:\n",
    "#           raise Exception('Length mismatch. Expected: {}. Found: {}'.format(self.length, len(t)))\n",
    "#         data[f.replace('.npy', '')] = t\n",
    "        \n",
    "#       return data\n",
    "#     self.inputs = load_data('input')\n",
    "#     self.outputs = load_data('output')\n",
    "\n",
    "#   def __len__(self):\n",
    "#     return len(self.inputs[0])\n",
    "\n",
    "#   def __getitem__(self, idx):\n",
    "#     def get_line(data):\n",
    "#       return {k: v[idx] for k,v in data.items()}\n",
    "#     return get_line(self.inputs), get_line(self.outputs)\n",
    "\n",
    "# move_dataset = MovementDataSet('data/only_side_moves')\n",
    "# print(move_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set([1,2,3,4])\n",
    "a.difference([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from collections import OrderedDict\n",
    "\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "\n",
    "def load_tensor(*segments):\n",
    "  t = th.tensor(np.load(os.path.join(*segments)).astype(np.float32))\n",
    "  return t\n",
    "\n",
    "\n",
    "def get_slice(data_dict: dict[str, th.Tensor], idx):\n",
    "  return {k: v[idx] for k,v in data_dict.items()}\n",
    "\n",
    "def get_avg(data_dict: dict[str, th.Tensor], keys: Optional[list[str]]):\n",
    "  r = {}\n",
    "  for k,v in data_dict.items():\n",
    "    if keys==None or keys and k in keys:\n",
    "      r[k] = v.mean(dim=0).item()\n",
    "  return r\n",
    "\n",
    "def get_std(data_dict: dict[str, th.Tensor], keys: Optional[list[str]]):\n",
    "  r = {}\n",
    "  for k,v in data_dict.items():\n",
    "    if keys==None or keys and k in keys:\n",
    "      r[k] = v.std(dim=0).item()\n",
    "  return r\n",
    "\n",
    "def view(data_dict: dict, fn_dict: Optional[dict] = None) -> OrderedDict:\n",
    "  if not fn_dict:\n",
    "    return OrderedDict(data_dict)\n",
    "  r = OrderedDict()\n",
    "  for k, v in data_dict.items():\n",
    "    if k not in fn_dict:\n",
    "      continue\n",
    "    fn = fn_dict[k]\n",
    "    if fn:\n",
    "      r[k] = fn(v)\n",
    "    else:\n",
    "      r[k] = v\n",
    "    continue\n",
    "  return r\n",
    "\n",
    "\n",
    "def load_all_data(data_path: str) -> Tuple[OrderedDict, OrderedDict]:\n",
    "  def load(type: str) -> OrderedDict:\n",
    "    dir_path: str = os.path.join(data_path, type) # ex: data/inputs\n",
    "    data = OrderedDict()\n",
    "    for f in os.listdir(dir_path):\n",
    "      data[f.replace('.npy', '')] = load_tensor(dir_path, f)\n",
    "    return data\n",
    "  return load('input'), load('output')\n",
    "    \n",
    "    \n",
    "def load_simple_move_data(data_path: str) -> Tuple[OrderedDict, OrderedDict]:\n",
    "  inputs = OrderedDict({\n",
    "    'dir': load_tensor(data_path, 'input', 'dir.npy')[:, 0],\n",
    "    'vel': load_tensor(data_path, 'input', 'vel.npy')[:, 0]\n",
    "  })\n",
    "  outputs = OrderedDict({\n",
    "    'dpos': load_tensor(data_path, 'output', 'dpos.npy')[:, 0],\n",
    "    'vel': load_tensor(data_path, 'output', 'vel.npy')[:, 0]\n",
    "  })\n",
    "  return inputs, outputs\n",
    "  \n",
    "  \n",
    "def write_same_line(*args):\n",
    "  sys.stdout.write(''.join(args))\n",
    "        \n",
    "   \n",
    "class KeyValueDataset(Dataset):\n",
    "  def __init__(self, inputs, outputs):\n",
    "    self.inputs = inputs\n",
    "    self.outputs = outputs\n",
    "    self.length = 0\n",
    "    for v in inputs.values():\n",
    "      self.length = len(v)\n",
    "      break\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self.length\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return get_slice(self.inputs, idx), get_slice(self.outputs, idx)\n",
    "  \n",
    "  def select_indexes(self, fn):\n",
    "    idx = []\n",
    "    for i in range(len(self)):\n",
    "      inputs, outputs = self[i]\n",
    "      if fn(inputs, outputs):\n",
    "        idx.append(i)\n",
    "    return idx\n",
    "  \n",
    "  def select_indexes_parallel(self, fn):\n",
    "    evaluation = fn(self.inputs, self.outputs)\n",
    "    idx: th.Tensor = th.nonzero(evaluation)\n",
    "    return idx.view(len(idx)).tolist()\n",
    "    \n",
    "  def select(self, fn):\n",
    "    return KeyValueDataset(*self[self.select_indexes(fn)])\n",
    "  \n",
    "  def select_parallel(self, fn):\n",
    "    return KeyValueDataset(*self[self.select_indexes_parallel(fn)])\n",
    "  \n",
    "  def random_split(self, prop, generator=None):\n",
    "    a, b = random_split(self, prop, generator)\n",
    "    return KeyValueDataset(*a[:]), KeyValueDataset(*b[:])\n",
    "  \n",
    "  def condition_split(self, fn):\n",
    "    idx = self.select_indexes(fn)\n",
    "    idx_ = list(set(range(len(self))).difference(idx))\n",
    "    return KeyValueDataset(*self[idx]), KeyValueDataset(*self[idx_])\n",
    "  \n",
    "  def condition_split_parallel(self, fn):\n",
    "    idx = self.select_indexes_parallel(fn)\n",
    "    idx_ = list(set(range(len(self))).difference(idx))\n",
    "    return KeyValueDataset(*self[idx]), KeyValueDataset(*self[idx_])\n",
    "  \n",
    "  def view(self, input_view=None, output_view=None):\n",
    "    return KeyValueDataset(view(self.inputs, input_view), view(self.outputs, output_view))\n",
    "  \n",
    "  def avg(self, input_keys=None, output_keys=None):\n",
    "    return get_avg(self.inputs, input_keys), get_avg(self.outputs, output_keys)\n",
    "  \n",
    "  def std(self, input_keys=None, output_keys=None):\n",
    "    return get_std(self.inputs, input_keys), get_std(self.outputs, output_keys)\n",
    "  \n",
    "  def no_repetition(self):\n",
    "    seen = set()\n",
    "    idx = []\n",
    "    for i in range(len(self)):\n",
    "      inputs, _ = self[i]\n",
    "      a = []\n",
    "      for v in inputs.values():\n",
    "        a.append(v.item())\n",
    "      key = tuple(a)\n",
    "      if key not in seen:\n",
    "        idx.append(i)\n",
    "        seen.add(key)\n",
    "    return KeyValueDataset(*self[idx])\n",
    "  \n",
    "  def range(self, start=None, end=None):\n",
    "    if not start:\n",
    "      start = 0\n",
    "    if end == None:\n",
    "      end = len(self)\n",
    "    start %= len(self)\n",
    "    end %= len(self)\n",
    "    return KeyValueDataset(*self[start: end])\n",
    "  \n",
    "  def print_yaml_like(self):\n",
    "    def print_data(data):\n",
    "      print(data)\n",
    "      for k, v in data.items():\n",
    "        print(k)\n",
    "        print(v)\n",
    "    for inputs, outputs in self:\n",
    "      print('---')\n",
    "      print_data(inputs)\n",
    "      print()\n",
    "      print_data(outputs)\n",
    "      \n",
    "  def print_table(self):\n",
    "    def print_header(keys):\n",
    "      for k in keys:\n",
    "        write_same_line((k).rjust(7))\n",
    "        write_same_line(' ')\n",
    "    \n",
    "    print_header(self.inputs.keys())\n",
    "    write_same_line('|')\n",
    "    print_header(self.outputs.keys())\n",
    "    write_same_line('\\n')\n",
    "    \n",
    "    def print_data(data):\n",
    "      write_same_line(*['{:.4f}'.format(v.item()).rstrip('0').rjust(7) + ' ' for v in data.values()])\n",
    "      # write_same_line(*['{}'.format(v.item()) + ' ' for v in data.values()])\n",
    "    for inputs, outputs in self:\n",
    "      print_data(inputs)\n",
    "      write_same_line(' ')\n",
    "      print_data(outputs)\n",
    "      sys.stdout.write('\\n')\n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "move_dataset = KeyValueDataset(*load_all_data('data/only_side_moves')).view({\n",
    "    'vel': lambda x: x[:, 0],\n",
    "    'dir': lambda x: x[:, 0]\n",
    "  },{\n",
    "    'vel': lambda x: x[:, 0],\n",
    "    'dpos': lambda x: x[:, 0]\n",
    "  })\n",
    "\n",
    "\n",
    "# move_dataset.print_table()\n",
    "# move_dataset.print_yaml_like()\n",
    "# print(move_dataset.avg())\n",
    "# print(move_dataset.std())\n",
    "\n",
    "# nodir_dataseet, dir_dataset = move_dataset.condition_split(lambda inputs, outputs: inputs['dir']==0)\n",
    "nodir_dataseet, dir_dataset = move_dataset.condition_split_parallel(lambda inputs, outputs: inputs['dir']==0)\n",
    "print(nodir_dataseet.avg(['dir'], []))\n",
    "print(nodir_dataseet.std(['dir'], []))\n",
    "print(dir_dataset.avg(['dir'], []))\n",
    "print(dir_dataset.std(['dir'], []))\n",
    "print(len(move_dataset))\n",
    "print(len(nodir_dataseet))\n",
    "print(len(dir_dataset))\n",
    "\n",
    "# move_dataset.condition_split_parallel(lambda inputs, outputs: inputs['dir']==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# inputs, outputs = move_dataset[:]\n",
    "# for e in zip(inputs['vel'], outputs['dpos']):\n",
    "#   print(*['{:.4f}'.format(ee.item()) for ee in e])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "inputs, outputs = move_dataset[:]\n",
    "# x, idx = th.sort(inputs['vel'])\n",
    "# y = th.gather(outputs['vel'], dim=0, index=idx)\n",
    "\n",
    "# for e in zip(x, y):\n",
    "#   print(*[ee.item() for ee in e])\n",
    "  \n",
    "ax.scatter(inputs['vel'], outputs['vel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "class Vint:\n",
    "  def __call__(self, v: Optional[int]=None) -> int:\n",
    "    if v:\n",
    "      self.v: int = v\n",
    "      return v\n",
    "    \n",
    "    if hasattr(self, 'v'):\n",
    "      return self.v\n",
    "    \n",
    "    return 0\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_reduce(a, kfn, redfn):\n",
    "  m = {}\n",
    "  for x in a:\n",
    "    k = kfn(x)\n",
    "    elem = m.get(k, None)\n",
    "    m[k] = redfn(elem, x)\n",
    "  return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "def save_if_better(id, min_loss, state_dict):\n",
    "  # print('minloss', min_loss)\n",
    "  if id in saved_models and min_loss >= saved_models[id]['loss']:\n",
    "    return\n",
    "  # print('Saving better model')\n",
    "  saved_models[id] = {\n",
    "    'loss': min_loss,\n",
    "    'state': state_dict\n",
    "  }\n",
    "\n",
    "def create_model(mtype, load_state = True, **kargs):\n",
    "  model = mtype(**kargs)\n",
    "  if load_state and model.id in saved_models:\n",
    "    saved = saved_models[model.id]\n",
    "    try:\n",
    "      model.load_state_dict(saved['state'])\n",
    "      print('Loaded model state. loss:', saved['loss'])\n",
    "    except:\n",
    "      print('Saved state doesn\\'t match model')\n",
    "  return model\n",
    "\n",
    "def get_lr(optimizer):\n",
    "  for param_group in optimizer.param_groups:\n",
    "    return float(param_group['lr'])\n",
    "  raise Exception('No earning rate found')\n",
    "     \n",
    "def fit(model, X, Y, Xval, Yval, epochs=1000, optimizer=None, lossfn: Callable=th.nn.MSELoss(), patiance = 50, min_lr=1e-9):\n",
    "  losses = []\n",
    "  val_losses = []\n",
    "  min_val_loss = 9999999999999999\n",
    "  best_state = None\n",
    "  try:  \n",
    "    if not optimizer:\n",
    "      optimizer = th.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "      \n",
    "    lrDecaySch = ReduceLROnPlateau(optimizer, patience=patiance, verbose=True, eps=min_lr*0.1, threshold=1e-4)\n",
    "    stop = False\n",
    "    lr = get_lr(optimizer)\n",
    "    print('Learning rate: ', lr)\n",
    "    \n",
    "    def get_loss(X, Y):\n",
    "      if isinstance(X, dict):\n",
    "        outputs = model(**X)\n",
    "      else:\n",
    "        outputs = model(X)\n",
    "      return lossfn(Y, outputs)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "      if stop:\n",
    "        break\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      loss = get_loss(X, Y)\n",
    "      val_loss = get_loss(Xval, Yval)\n",
    "      if i%10 == 0:\n",
    "        print('epoch {}, lr: {:.4}, loss {:.6}, val_loss: {:.6}'.format(i, lr, loss.item(), val_loss.item()))\n",
    "      \n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "      lrDecaySch.step(loss)\n",
    "      lr = get_lr(optimizer)\n",
    "      if lr < min_lr:\n",
    "        stop=True\n",
    "      \n",
    "      curr_val_loss = val_loss.item()\n",
    "      losses.append(loss.item())\n",
    "      val_losses.append(curr_val_loss)\n",
    "      if curr_val_loss < min_val_loss:\n",
    "        min_val_loss = curr_val_loss\n",
    "        best_state = model.state_dict()\n",
    "        save_if_better(model.id, min_val_loss, best_state)\n",
    "  except KeyboardInterrupt:\n",
    "    print('Training interrupted')\n",
    "  return losses, val_losses, min_val_loss, best_state\n",
    "\n",
    "def plot_loss(loss, label, ax=None):\n",
    "  if not ax:\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "  loss = [(l) for l in loss]\n",
    "  ax.plot(range(len(loss)), loss, lw=0.5, label=label)\n",
    "  ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSideMoveStopVel(nn.Module):\n",
    "  def __init__(self, n):\n",
    "    super(ModelSideMoveStopVel, self).__init__()\n",
    "    self.lin1 = nn.Linear(1, n)\n",
    "    self.lin2 = nn.Linear(n, 1, bias=False)\n",
    "    \n",
    "\n",
    "  def forward(self, vel):\n",
    "    x = self.lin1(vel[:, None])\n",
    "    x = th.relu(x)\n",
    "    x = self.lin2(x)\n",
    "    return x.view(-1)\n",
    "\n",
    "# class ModelSideMoveStopVel(nn.Module):\n",
    "#   def __init__(self):\n",
    "#     super(ModelSideMoveStopVel, self).__init__()\n",
    "#     self.cap1 = nn.Parameter(th.tensor(0.2))\n",
    "#     self.cap2 = nn.Parameter(th.tensor(0.2))\n",
    "    \n",
    "\n",
    "#   def forward(self, vel):\n",
    "#     return th.relu(vel + self.cap1) - th.relu(-vel + self.cap2)\n",
    "    \n",
    "\n",
    "inputs, outputs = move_dataset[:]\n",
    "\n",
    "# inputs['vel'] = inputs['vel'][:, None]\n",
    "# outputs['vel'] = outputs['vel'][:, None]\n",
    "with th.no_grad():\n",
    "  model = ModelSideMoveStopVel(8)\n",
    "  preds = model(inputs['vel'])\n",
    "  print('preds:', preds.shape)\n",
    "  print('outputs:', outputs['vel'].shape)\n",
    "  print('loss:', th.nn.MSELoss()(preds, outputs['vel']))\n",
    "  print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_model(Model2)\n",
    "model_vel = ModelSideMoveStopVel(4)\n",
    "losses, min_loss, best_state = fit(model_vel, inputs['vel'], outputs['vel'], lossfn=nn.L1Loss(), epochs=10000, optimizer=th.optim.Adam(model_vel.parameters(), lr=0.1))\n",
    "# save_if_better(type(model), min_loss, best_state)\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_vel.lin1.weight = nn.Parameter(th.tensor([[1], [-1]], dtype=th.float32))\n",
    "# model_vel.lin1.bias = nn.Parameter(th.tensor([0.4, 0.1], dtype=th.float32))\n",
    "# model_vel.lin2.weight = nn.Parameter(th.tensor([[0.1,  0.5]], dtype=th.float32))\n",
    "# print(model_vel.state_dict())\n",
    "\n",
    "def plot_fit(model, inputs, outputs):\n",
    "  with th.no_grad():\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(inputs, outputs)\n",
    "    v0 = th.arange(-2, 2, 0.1)\n",
    "    v1 = model(v0)\n",
    "    ax.plot(v0, v1, color='#C44')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_histogram(model, inputs, outputs, res = 0.1, ax=None, title=''):\n",
    "  with th.no_grad():\n",
    "    if isinstance(inputs, dict):\n",
    "      pred = model(**inputs)\n",
    "    else:\n",
    "      pred = model(inputs)\n",
    "    e = pred - outputs\n",
    "    hist = map_reduce(e, lambda x: int(x/res)*res, lambda acc, x: acc+1 if acc else 1)\n",
    "\n",
    "    if not ax:\n",
    "      fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.set_title(' '.join((title, model.id)))\n",
    "    ax.set_ylabel('%')\n",
    "    ax.set_xlabel('error')\n",
    "    # print(hist.keys())\n",
    "    ax.bar(hist.keys(), [v/len(e) * 100 for v in hist.values()], width=res)\n",
    "    ax.set_xlim(min(hist.keys()) -res -1, max(hist.keys())+res+1)\n",
    "\n",
    "# plot_error_histogram(model_vel, inputs['vel'], outputs['vel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with th.no_grad():\n",
    "  def within_error(inputs, outputs):\n",
    "    e = model_vel(inputs['vel'][None]) - outputs['vel']\n",
    "    l = e**2\n",
    "    return l.item() < 0.3\n",
    "\n",
    "  filtered_dataset = move_dataset.select(within_error)\n",
    "\n",
    "  inputs, outputs = filtered_dataset[:]\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.scatter(inputs['vel'], outputs['vel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, min_loss, best_state = fit(model_vel, inputs['vel'], outputs['vel'], lossfn=nn.L1Loss(), epochs=10000, optimizer=th.optim.Adam(model_vel.parameters(), lr=0.1))\n",
    "# save_if_better(type(model), min_loss, best_state)\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pos = ModelSideMoveStopVel(8)\n",
    "losses, min_loss, best_state = fit(model_pos, inputs['vel'], outputs['dpos'], lossfn=nn.L1Loss(), epochs=10000, optimizer=th.optim.Adam(model_pos.parameters(), lr=0.1))\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(model_pos, inputs['vel'], outputs['dpos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_histogram(model_pos, inputs['vel'], outputs['dpos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_dataset = KeyValueDataset(*load_all_data('data/only_side_moves')).view({\n",
    "    'vel': lambda x: x[:, 0],\n",
    "    'dir': lambda x: x[:, 0]\n",
    "  },{\n",
    "    'vel': lambda x: x[:, 0],\n",
    "    'dpos': lambda x: x[:, 0]\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_dataset.print_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSideMoveVel(nn.Module):\n",
    "  def __init__(self, n, act=th.relu):\n",
    "    super(ModelSideMoveVel, self).__init__()\n",
    "    self.lin1 = nn.Linear(2, n)\n",
    "    self.lin2 = nn.Linear(n, 1, bias=False)\n",
    "    self.act = act\n",
    "    \n",
    "\n",
    "  def forward(self, vel, dir):\n",
    "    x = th.cat([vel[:, None], dir[:, None]], dim=1)\n",
    "    x = self.lin1(x)\n",
    "    x = self.act(x)\n",
    "    x = self.lin2(x)\n",
    "    return x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vel = ModelSideMoveVel(4, th.relu)\n",
    "losses, min_loss, best_state = fit(model_vel, inputs, outputs['vel'], lossfn=nn.L1Loss(), epochs=10000, \n",
    "                                   optimizer=th.optim.Adam(model_vel.parameters(), lr=0.1))\n",
    "# save_if_better(type(model), min_loss, best_state)\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_histogram(model_vel, inputs, outputs['vel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_dataset = KeyValueDataset(*load_all_data('data/side_moves_jump')).view({\n",
    "    'vel': lambda x: x[:, 0],\n",
    "    'dir': lambda x: x[:, 0],\n",
    "    'jump': None,\n",
    "    'onledge': None,\n",
    "    'wall': None\n",
    "  },{\n",
    "    'vel': lambda x: x[:, 0],\n",
    "    'dpos': lambda x: x[:, 0]\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# generator = th.Generator().manual_seed(130)\n",
    "train_dataset, val_dataset = move_dataset.random_split([0.7, 0.3])\n",
    "X_train, Y_train = train_dataset[:]\n",
    "X_val, Y_val = val_dataset[:]\n",
    "\n",
    "print()\n",
    "print(train_dataset.avg(['vel'], ['dpos']))\n",
    "print(val_dataset.avg(['vel'], ['dpos']))\n",
    "print(train_dataset.std(['vel'], ['dpos']))\n",
    "print(val_dataset.std(['vel'], ['dpos']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSideMoveVel(nn.Module):\n",
    "  def __init__(self, dims, act = th.nn.ReLU):\n",
    "    super(ModelSideMoveVel, self).__init__()\n",
    "    self.layers = nn.Sequential()\n",
    "    for i, d in enumerate(dims):\n",
    "      self.layers.append(nn.LazyLinear(d))\n",
    "      self.layers.append(act())\n",
    "      self.layers.append(nn.LazyBatchNorm1d())\n",
    "    \n",
    "    self.last = nn.LazyLinear(1)\n",
    "    self.act = act\n",
    "    self.id = '_'.join(['ModelSideMoveVel', *[str(d) for d in dims], str(act.__name__)])\n",
    "    \n",
    "  def forward(self, vel, dir, wall, onledge, jump):\n",
    "    vel = vel[:, None]\n",
    "    # print(vel.shape)\n",
    "    dir = dir[:, None]\n",
    "    # print(dir.shape)\n",
    "    onledge = onledge[:, None]\n",
    "    # print(onledge.shape)\n",
    "    wall = wall.view(len(wall), -1)\n",
    "    # print(wall.shape)\n",
    "    input = th.cat([vel, dir, wall, onledge, jump], dim=1)\n",
    "    # x = input\n",
    "    # for l in self.layers:\n",
    "    #   x = l(x)\n",
    "    #   x = self.act(x)\n",
    "    #   print(x.shape)\n",
    "    #   x = th.cat([input, x], dim=1)\n",
    "    x = self.layers(input)\n",
    "    x = self.last(x)\n",
    "    \n",
    "    return x.view(-1)\n",
    "\n",
    "model = ModelSideMoveVel([16, 16], act=th.nn.ReLU)\n",
    "# model.to('cuda')\n",
    "print(model.id)\n",
    "plot_error_histogram(model, X_val, Y_val['dpos'], res=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(ModelSideMoveVel, dims=[32], act=th.nn.ReLU, load_state=True)\n",
    "device = 'cpu'\n",
    "# model.to(device)\n",
    "# def move_to_device(data, device):\n",
    "#   for k, v in data.items():\n",
    "#     data[k] = v.to(device)\n",
    "    \n",
    "# move_to_device(X_train, device)\n",
    "# move_to_device(Y_train, device)\n",
    "# move_to_device(X_val, device)\n",
    "# move_to_device(Y_val, device)\n",
    "\n",
    "# print(model.state_dict())\n",
    "# print(th.nn.L1Loss()(model(**Xval), Yval['vel']))\n",
    "\n",
    "var = 'dpos'\n",
    "# del saved_models[model.id]\n",
    "losses, val_losses, min_loss, best_state = fit(model, X_train, Y_train[var], X_val, Y_val[var], lossfn=nn.L1Loss(), epochs=10000, \n",
    "    optimizer=th.optim.Adam(model.parameters(), lr=0.1))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "  \n",
    "plot_loss(losses, 'train', ax)\n",
    "plot_loss(val_losses, 'val', ax)\n",
    "# print(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "model = create_model(ModelSideMoveVel, dims=[32], act=th.nn.ReLU)\n",
    "model.eval()\n",
    "# model.to('cuda')\n",
    "plot_error_histogram(model, X_val, Y_val[var], title='val', res=0.01, ax=ax[0])\n",
    "plot_error_histogram(model, X_train, Y_train[var], title='train', res=0.01, ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def within_error(inputs, outputs):\n",
    "  e = model(**inputs) - outputs[var]\n",
    "  # print(e.shape)\n",
    "  l = abs(e)\n",
    "  r = l < 0.01\n",
    "  # print(len(r))\n",
    "  return r\n",
    "\n",
    "# print(len(train_dataset))\n",
    "l_train_dataset, h_train_dataset = train_dataset.condition_split_parallel(within_error)\n",
    "l_val_dataset, h_val_dataset = val_dataset.condition_split_parallel(within_error)\n",
    "\n",
    "# print(len(l_train_dataset))\n",
    "# print(len(h_train_dataset))\n",
    "# print(len(l_val_dataset))\n",
    "# print(len(h_val_dataset))\n",
    "\n",
    "X, Y = l_train_dataset[:]\n",
    "print(X['wall'].shape)\n",
    "plot_error_histogram(model, X, Y[var], title='val', res=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_h_train, Y_h_train = h_train_dataset[:]\n",
    "X_h_val, Y_h_val = h_val_dataset[:]\n",
    "model = create_model(ModelSideMoveVel, dims=[4], act=th.nn.ReLU)\n",
    "losses, val_losses, min_loss, best_state = fit(model, X_h_train, Y_h_train['dpos'], X_val, Y_val[var], lossfn=nn.L1Loss(), epochs=10000, \n",
    "    optimizer=th.optim.Adam(model.parameters(), lr=0.1))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "  \n",
    "plot_loss(losses, 'train', ax)\n",
    "plot_loss(val_losses, 'val', ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "model = create_model(ModelSideMoveVel, dims=[4], act=th.nn.ReLU)\n",
    "model.eval()\n",
    "# model.to('cuda')\n",
    "plot_error_histogram(model, X_val, Y_val[var], title='val', res=0.01, ax=ax[0])\n",
    "plot_error_histogram(model, X_train, Y_train[var], title='train', res=0.01, ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
