{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "maxv = 50\n",
    "\n",
    "# This model simulates a falling body that reaches a maximum speed.\n",
    "# The non linearity on the maximum speed is intentional. This notebook\n",
    "# experiments with many models that catches this non-linearity.\n",
    "class ReferenceModel(th.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ReferenceModel, self).__init__()\n",
    "    g = -9.8\n",
    "    dt = 0.016\n",
    "    self.A = th.tensor([[1, dt], [0, 1]], requires_grad=False).T\n",
    "    self.b = th.tensor([0, dt * g], requires_grad=False).T\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    x = inputs @ self.A + self.b\n",
    "    x[:, 1] = th.clamp(x[:, 1], -maxv, maxv)\n",
    "    return x\n",
    "\n",
    "ref_model = ReferenceModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trajectory(model, vel):  \n",
    "  '''Creates a trajectory with a starting velocity vel'''\n",
    "  \n",
    "  with th.no_grad():\n",
    "    first = th.tensor([[0, vel]], dtype=th.float32)\n",
    "    X = [first]\n",
    "    for i in range(800):\n",
    "      x0 = X[-1]\n",
    "      X.append(ref_model(x0))\n",
    "\n",
    "    return th.cat(X)\n",
    "\n",
    "X = create_trajectory(ref_model, maxv)\n",
    "\n",
    "def plot_trajectory(X, ax = None):\n",
    "  if not ax:\n",
    "    fig, ax = plt.subplots()\n",
    "  ax.plot(range(len(X)), [0] * len(X), ':', color='#F0F0F0')\n",
    "  ax.plot(range(len(X)), [-maxv] * len(X), ':', color='#F0F0F0')\n",
    "  ax.plot(range(len(X)), [x[0] for x in X], color=\"#8080FF\", label='pos')\n",
    "  ax.plot(range(len(X)), [x[1] for x in X], color='#FF8044', label='vel')\n",
    "  ax.legend()\n",
    "\n",
    "plot_trajectory(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "def flatten(l):\n",
    "  r = []\n",
    "  for i in l:\n",
    "    if isinstance(i, Iterable):\n",
    "      r.extend(flatten(i))\n",
    "    else:\n",
    "      r.append(i)\n",
    "  return r\n",
    "\n",
    "flatten([[1, 2 , [3, 5, 2]], [2]])\n",
    "# print(type([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = []\n",
    "Ytrain = []\n",
    "i = 0\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(12, 6))\n",
    "axs = flatten(axs)\n",
    "\n",
    "# Creates a data set with several different starting velocities\n",
    "for v in th.arange(-0.9, 0.901, 0.2):\n",
    "  v *= maxv\n",
    "  X = create_trajectory(ref_model, v)\n",
    "  if i < 9:\n",
    "    plot_trajectory(X, axs[i])\n",
    "    i += 1\n",
    "  Xtrain.append(X[:-1])\n",
    "  Ytrain.append(X[1:])\n",
    "\n",
    "Xtrain = th.cat(Xtrain)\n",
    "Ytrain = th.cat(Ytrain)\n",
    "\n",
    "print('Xtrain:', Xtrain.shape)\n",
    "print('Ytrain:', Ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_reduce(a, kfn, redfn):\n",
    "  m = {}\n",
    "  for x in a:\n",
    "    k = kfn(x)\n",
    "    elem = m.get(k, None)\n",
    "    m[k] = redfn(elem, x)\n",
    "  return m\n",
    "\n",
    "def red_count(acc, x):\n",
    "  if acc == None:\n",
    "    return 1\n",
    "  return acc + 1\n",
    "\n",
    "hist_vel = map_reduce(Xtrain, lambda x: int(x[1]), red_count)\n",
    "hist_pos = map_reduce(Xtrain, lambda x: int(x[1]/4), red_count)\n",
    "\n",
    "def plot_histogram(hist, label, ax=None):\n",
    "  if not ax:\n",
    "    fig, ax = plt.subplots()\n",
    "  ax.set_title(label + ' count')\n",
    "  ax.bar(hist.keys(), hist.values())\n",
    "  ax.set_ylabel('count')\n",
    "  ax.set_xlabel(label)\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(10, 3))\n",
    "plot_histogram(hist_vel, 'vel', axs[0])\n",
    "plot_histogram(hist_pos, 'pos/4', axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del models\n",
    "saved_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(th.nn.Module):\n",
    "  '''This exploits knowledge from he physical system. Since we know what parameters exists and how they\n",
    "    affect the system, we can do simple linear regression.'''\n",
    "  def __init__(self):\n",
    "    super(Model1, self).__init__()\n",
    "    self.dt = th.nn.Parameter(th.rand(1))\n",
    "    self.g = th.nn.Parameter(th.rand(1))\n",
    "    self.maxv = th.nn.Parameter(th.rand(1))\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    vel = inputs[:, 1]\n",
    "    pos = inputs[:, 0]\n",
    "    pos = vel*self.dt + pos\n",
    "    vel = self.g*self.dt + vel\n",
    "    vel = th.clamp(vel, -self.maxv, self.maxv)\n",
    "    return th.stack((pos, vel), dim=1)\n",
    "\n",
    "\n",
    "def create_model(mtype, load_state = True):\n",
    "  model = mtype()\n",
    "  if load_state and str(mtype) in saved_models:\n",
    "    saved = saved_models[str(mtype)]\n",
    "    try:\n",
    "      model.load_state_dict(saved['state'])\n",
    "      print('Loaded model state. loss:', saved['loss'])\n",
    "    except:\n",
    "      print('Saved state doesn\\'t match model')\n",
    "  return model\n",
    "\n",
    "print(saved_models.keys())\n",
    "model1 = create_model(Model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def get_lr(optimizer):\n",
    "  for param_group in optimizer.param_groups:\n",
    "    return float(param_group['lr'])\n",
    "  raise Exception('No earning rate found')\n",
    "      \n",
    "def fit(model, X=Xtrain, Y=Ytrain, batch_size=None, epochs=1000, optimizer=None, lossfn=th.nn.L1Loss(), patiance = 100, min_lr=1e-9):\n",
    "  losses = []\n",
    "  # noimp = 0\n",
    "  min_loss = 9999999999999999\n",
    "  best_state = None\n",
    "  if not optimizer:\n",
    "    optimizer = th.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "  lrDecaySch = ReduceLROnPlateau(optimizer, patience=patiance, verbose=True, eps=min_lr*0.1, threshold=1e-6)\n",
    "  stop = False\n",
    "  lr = get_lr(optimizer)\n",
    "  print('Learning rate: ', lr)\n",
    "  for i in range(epochs):\n",
    "    if stop:\n",
    "      break\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if batch_size:\n",
    "      idx = th.randint(0, len(X), (batch_size,))\n",
    "      Xtrain = X[idx]\n",
    "      Ytrain = X[idx]\n",
    "    else:\n",
    "      Xtrain = X\n",
    "      Ytrain = Y\n",
    "    \n",
    "    outputs = model(Xtrain)\n",
    "    \n",
    "    loss = lossfn(Ytrain, outputs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('epoch {}, lr: {}, loss {}'.format(i, lr, loss.item()))\n",
    "    lrDecaySch.step(loss)\n",
    "    lr = get_lr(optimizer)\n",
    "    if lr < min_lr:\n",
    "      stop=True\n",
    "        \n",
    "    curr_loss = loss.item()\n",
    "    losses.append(curr_loss)\n",
    "    if curr_loss < min_loss:\n",
    "      # noimp = 0\n",
    "      min_loss = curr_loss\n",
    "      best_state = model.state_dict()\n",
    "    # else:\n",
    "    #   noimp += 1\n",
    "    #   if noimp >= patiance:\n",
    "    #     break\n",
    "  print('Learning rate: ', lr)\n",
    "  return losses, min_loss, best_state\n",
    "\n",
    "import math\n",
    "\n",
    "def plot_loss(losses):\n",
    "  fig, ax = plt.subplots(figsize=(10, 2.5))\n",
    "  ax.plot(range(len(losses)), [math.log(l) for l in losses])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_if_better(mtype, min_loss, state_dict):\n",
    "  print('minloss', min_loss)\n",
    "  if str(mtype) in saved_models and min_loss >= saved_models[str(mtype)]['loss']:\n",
    "    return\n",
    "  print('Saving better model')\n",
    "  saved_models[str(mtype)] = {\n",
    "    'loss': min_loss,\n",
    "    'state': state_dict\n",
    "  }\n",
    "\n",
    "model1 = create_model(Model1)\n",
    "losses, min_loss, best_state = fit(model1, epochs=2000, optimizer=th.optim.AdamW(model1.parameters(), lr=0.01))\n",
    "save_if_better(type(model1), min_loss, best_state)\n",
    "plot_loss(losses)\n",
    "print(model1.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_plots(ma, mb, vel, ax=None):\n",
    "  with th.no_grad():\n",
    "    first = th.tensor([[0, vel]], dtype=th.float32)\n",
    "    Xa = [first]\n",
    "    Xb = [first]\n",
    "    for i in range(1000):\n",
    "      xa = Xa[-1]\n",
    "      # [None, :]\n",
    "      # print(xa.shape)\n",
    "      Xa.append(ma(Xa[-1]))\n",
    "      Xb.append(mb(Xb[-1]))\n",
    "    \n",
    "    Xa = th.cat(Xa)\n",
    "    Xb = th.cat(Xb)\n",
    "    if not ax:\n",
    "      fig, ax = plt.subplots(figsize=(5, 2.5))\n",
    "    img = ax.plot(range(len(X)), [0] * len(X), ':', color='#F0F0F0')\n",
    "    img = ax.plot(range(len(X)), [-maxv] * len(X), ':', color='#F0F0F0')\n",
    "    def plot(X, style='-'):\n",
    "      img = ax.plot(range(len(X)), [x[0] for x in X], style, color=\"#8080FF\", label='pos')\n",
    "      img = ax.plot(range(len(X)), [x[1] for x in X], style, color='#FF8044', label='vel')\n",
    "    plot(Xa, ':')\n",
    "    plot(Xb)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_many(ma, mb):\n",
    "  fig, axs = plt.subplots(3, 3, figsize=(14, 9))\n",
    "  axs = flatten(axs)\n",
    "\n",
    "  for i, v in enumerate(th.arange(-0.85, 1.01, 0.23)):\n",
    "    v *= maxv\n",
    "    axs[i].set_title(str(v.item()))\n",
    "    compare_plots(ma, mb, v, axs[i])\n",
    "\n",
    "compare_many(ref_model, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(th.nn.Module):\n",
    "  '''Model that tries to use tanh as non linearity for speed.\n",
    "     Since there is no linearity in the position, bypass that with a second linear layer and sum at the end'''\n",
    "  def __init__(self):\n",
    "    super(Model2, self).__init__()\n",
    "    self.lin1 = th.nn.Linear(2, 2)\n",
    "    self.lin2 = th.nn.Linear(2, 2)\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    x = th.tanh(self.lin1(inputs))\n",
    "    x = inputs + self.lin2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(Model2)\n",
    "losses, min_loss, best_state = fit(model, epochs=2000, optimizer=th.optim.AdamW(model.parameters(), lr=0.01))\n",
    "save_if_better(type(model), min_loss, best_state)\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_many(ref_model, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3(th.nn.Module):\n",
    "  '''Similar to model2, it tries to fit the non-linearity using tanh.\n",
    "     But this makes the nn more powerfull by adding extra weights on the last layer'''\n",
    "  def __init__(self):\n",
    "    super(Model3, self).__init__()\n",
    "    self.lin1 = th.nn.Linear(2, 2)\n",
    "    self.lin2 = th.nn.Linear(4, 2)\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    x = th.tanh(self.lin1(inputs))\n",
    "    cat = th.cat([inputs, x], dim=1)\n",
    "    x = self.lin2(cat)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(Model3)\n",
    "losses, min_loss, best_state = fit(model, epochs=2000, optimizer=th.optim.AdamW(model.parameters(), lr=1e-5))\n",
    "save_if_better(type(model), min_loss, best_state)\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_many(ref_model, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model4(th.nn.Module):\n",
    "  '''Trying to train 2 separate linear models by splitting the path with a softmax'''\n",
    "  def __init__(self):\n",
    "    super(Model4, self).__init__()\n",
    "    self.emb = th.nn.Linear(2, 2, bias = False)\n",
    "    self.lina = th.nn.Linear(2, 2)\n",
    "    self.linb = th.nn.Linear(2, 2)\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    emb = th.softmax(self.emb(inputs), dim = 1)\n",
    "    self.soft = emb\n",
    "    return emb[:,0].view(-1, 1)*self.lina(inputs) + emb[:,1].view(-1, 1)*self.linb(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(Model4)\n",
    "losses, min_loss, best_state = fit(model, epochs=2000, optimizer=th.optim.AdamW(model.parameters(), lr=1e-7))\n",
    "save_if_better(type(model), min_loss, best_state)\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = create_model(Model4)\n",
    "compare_many(ref_model, model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_soft(model, kfn):\n",
    "  '''Plot the softmax based on the mapping function kfn'''\n",
    "  with th.no_grad():\n",
    "    model(Xtrain)\n",
    "    soft: th.Tensor = th.cat([Xtrain, model.soft], dim=1)\n",
    "\n",
    "    soft_sum0 = map_reduce(soft, kfn, lambda acc, x: (acc[0]+1, acc[1]+x[2]) if acc else (1, x[2]))\n",
    "    soft_sum1 = map_reduce(soft, kfn, lambda acc, x: (acc[0]+1, acc[1]+x[3]) if acc else (1, x[3]))\n",
    "    \n",
    "    def avg(s):\n",
    "      r = {}\n",
    "      for k, v in s.items():\n",
    "        r[k] = v[1] / v[0]\n",
    "      return r\n",
    "    \n",
    "    soft_sum0 = avg(soft_sum0)\n",
    "    soft_sum1 = avg(soft_sum1)\n",
    "    \n",
    "    fig, (ax0, ax1) = plt.subplots(2)\n",
    "    ax0.bar(soft_sum0.keys(), soft_sum0.values())\n",
    "    ax1.bar(soft_sum1.keys(), soft_sum1.values())\n",
    "\n",
    "\n",
    "# This transition should happen strongly at -50, but instead starts happening mildly around -40\n",
    "plot_soft(model4, lambda x: int(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The position is tricking the embedding somehow. We know that position should not affect how we change velocity,\n",
    "# but the nn finds a way to correlate them\n",
    "plot_soft(model4, lambda x: int(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model5(th.nn.Module):\n",
    "  '''Trying to train 2 separate linear models by splitting the path with a softmax.\n",
    "     The difference now is making the split stronger'''\n",
    "  def __init__(self):\n",
    "    super(Model5, self).__init__()\n",
    "    self.emb = th.nn.Linear(2, 2)\n",
    "    self.lina = th.nn.Linear(2, 2)\n",
    "    self.linb = th.nn.Linear(2, 2)\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    emb = th.softmax(self.emb(inputs), dim = 1)\n",
    "    emb = emb**2\n",
    "    emb /= emb.sum(dim=1, keepdim=True)\n",
    "    self.soft = emb\n",
    "    return emb[:,0].view(-1, 1)*self.lina(inputs) + emb[:,1].view(-1, 1)*self.linb(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(Model5)\n",
    "losses, min_loss, best_state = fit(model, epochs=10000, optimizer=th.optim.AdamW(model.parameters(), lr=1e-7))\n",
    "save_if_better(type(model), min_loss, best_state)\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_many(ref_model, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = create_model(Model5)\n",
    "\n",
    "# Based on vel\n",
    "plot_soft(model5,  lambda x: int(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on pos\n",
    "plot_soft(model5,  lambda x: int(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model6(th.nn.Module):\n",
    "  '''Like model5, but lets try to improve the encoding. We know that the non linearity depends only on velocity, not position.\n",
    "     So lets exploit that.'''\n",
    "  def __init__(self):\n",
    "    super(Model6, self).__init__()\n",
    "    self.emb = th.nn.Linear(1, 2, bias=False)\n",
    "    self.lina = th.nn.Linear(2, 2)\n",
    "    self.linb = th.nn.Linear(2, 2)\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    emb = self.emb(inputs[:, 1][:, None])\n",
    "    soft = th.softmax(emb, dim = 1)\n",
    "    self.soft = soft\n",
    "    return soft[:,0].view(-1, 1)*self.lina(inputs) + soft[:,1].view(-1, 1)*self.linb(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(Model6, False)\n",
    "losses, min_loss, best_state = fit(model, epochs=10000, optimizer=th.optim.AdamW(model.parameters(), lr=1e-1))\n",
    "save_if_better(type(model), min_loss, best_state)\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_model(Model6)\n",
    "compare_many(ref_model, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = create_model(Model6)\n",
    "plot_soft(model6,  lambda x: int(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_soft(model6,  lambda x: int(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embs(model):\n",
    "  with th.no_grad():\n",
    "    model(Xtrain)\n",
    "    # soft = model.soft.argmax(dim=1)\n",
    "    # print('soft', soft.unique())\n",
    "\n",
    "    print('model.soft:', model.soft.shape)\n",
    "    soft: th.Tensor = th.cat([Xtrain, model.soft], dim=1)\n",
    "\n",
    "    print('soft:', soft.shape)\n",
    "\n",
    "    # def redfn(acc, x):\n",
    "    #   # print(x.shape)\n",
    "    #   return (acc if acc else 0)+x[2]\n",
    "\n",
    "    soft_sum0 = map_reduce(soft, lambda x: int(x[1]), lambda acc, x: (acc if acc else 0)+x[2])\n",
    "    soft_sum1 = map_reduce(soft, lambda x: int(x[1]), lambda acc, x: (acc if acc else 0)+x[3])\n",
    "    # soft_sum_total = {}\n",
    "    # def add(s):\n",
    "    #   for k, v in s.items():\n",
    "    #     if k not in soft_sum_total:\n",
    "    #       soft_sum_total[k] = 0\n",
    "    #     soft_sum_total[k] += v\n",
    "    # add(soft_sum0)\n",
    "    # add(soft_sum1)\n",
    "    \n",
    "    fig, (ax0, ax1) = plt.subplots(2)\n",
    "    \n",
    "    ax0.bar(soft_sum0.keys(), soft_sum0.values())\n",
    "    \n",
    "    ax1.bar(soft_sum1.keys(), soft_sum1.values())\n",
    "\n",
    "    # h_l = 2\n",
    "    # soft_h = th.zeros((2, maxv*2 // h_l + 1), requires_grad=False)\n",
    "    # for i, x in enumerate(Xtrain):\n",
    "    #   idx: int = int(x[1] / h_l) + maxv // h_l\n",
    "    #   soft_h[soft[i]][idx] += 1\n",
    "\n",
    "\n",
    "    # img = plt.plot(range(len(soft_h[1])), soft_h[1])\n",
    "    # img = plt.plot(range(len(soft_h[0])), soft_h[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(saved_models)\n",
    "for k, v in saved_models.items():\n",
    "  filename = 'models/' + k.replace('<class \\'__main__.', '').replace('\\'>', '')\n",
    "  print(filename)\n",
    "  th.save(v, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "smodels = {}\n",
    "for f in os.listdir('models'):\n",
    "  smodels[f] = th.load('models/'+f)\n",
    "\n",
    "print(smodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPos(th.nn.Module):\n",
    "  '''Trying to train 2 separate linear models by splitting the path with a softmax just for pos'''\n",
    "  def __init__(self):\n",
    "    super(ModelPos, self).__init__()\n",
    "    # self.emb = th.nn.Linear(2, 2, bias = False)\n",
    "    self.lina = th.nn.Linear(2, 1)\n",
    "    # self.linb = th.nn.Linear(2, 1)\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    # emb = th.softmax(self.emb(inputs), dim = 1)\n",
    "    # self.soft = emb\n",
    "    # return emb[:,0].view(-1, 1)*self.lina(inputs) + emb[:,1].view(-1, 1)*self.linb(inputs)\n",
    "    return self.lina(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(ModelPos)\n",
    "losses, min_loss, best_state = fit(model, Y=Ytrain[:, 0][:, None], epochs=1000, optimizer=th.optim.AdamW(model.parameters(), lr=1e-5))\n",
    "save_if_better(type(model), min_loss, best_state)\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelVel2(th.nn.Module):\n",
    "  '''Trying to train 2 separate linear models by splitting the path with a softmax just for vel'''\n",
    "  def __init__(self):\n",
    "    super(ModelVel2, self).__init__()\n",
    "    # self.query = th.nn.Linear(1, 2, bias = False)\n",
    "    # self.value = th.nn.Linear(, 1, bias = False)\n",
    "    self.lina = th.nn.Linear(1, 1)  \n",
    "    self.linb = th.nn.Linear(1, 1)\n",
    "    # self.dca = th.nn.Parameter(th.rand(1))\n",
    "    # self.dcb = th.nn.Parameter(th.rand(1))\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    # print(inputs.shape)\n",
    "    # q = self.query(inputs[:, 1:2])\n",
    "    # w = th.softmax(q, dim = -1)\n",
    "    # self.soft = w\n",
    "    # # print(w)\n",
    "    # # v = th.cat([self.lina(inputs), self.linb(inputs)], dim=1)\n",
    "    # v = th.cat([inputs[:, 1:2] - self.dca, inputs[:, 1:2] - self.dcb], dim=1)\n",
    "    # arg = th.argmax(w, dim=1)\n",
    "    # # print(v.shape)\n",
    "    # # print(v.view(-1, 2, 1))\n",
    "    # # result = w.view(-1, 1, 2) @ v.view(-1, 2, 1)\n",
    "    # result = arg\n",
    "    # print('result', result)\n",
    "    # return result.view(-1, 1)\n",
    "    vel = inputs[:, 1:2]\n",
    "    w0 = (vel < -50).type(th.float32)\n",
    "    w1 = 1 - w0\n",
    "    w = th.cat([w0, w1], dim=1)\n",
    "    # print(w)\n",
    "    # print(w.shape)\n",
    "    v = th.cat([self.lina(vel), self.linb(vel)], dim=1)\n",
    "    # print(v.shape)\n",
    "    result = w.view(-1, 1, 2) @ v.view(-1, 2, 1)\n",
    "    return result.view(-1, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del saved_models[str(ModelVel2)]\n",
    "model = create_model(ModelVel2, load_state=True)\n",
    "losses, min_loss, best_state = fit(model, Y=Ytrain[:, 1][:, None], batch_size=None, epochs=10000, optimizer=th.optim.AdamW(model.parameters(), lr=1e-2))\n",
    "save_if_better(type(model), min_loss, best_state)\n",
    "plot_loss(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.state_dict())\n",
    "for k,v in model.state_dict().items():\n",
    "  print(k)\n",
    "  print('  ',v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelVelPos(th.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ModelVelPos, self).__init__()\n",
    "    self.pos = create_model(ModelPos)\n",
    "    self.vel = create_model(ModelVel2)\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    p = self.pos(inputs)\n",
    "    v = self.vel(inputs)\n",
    "    r = th.cat([p, v], dim=1)\n",
    "    # print(r.shape)\n",
    "    return th.cat([p, v], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelVelPos()\n",
    "compare_many(ref_model, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
